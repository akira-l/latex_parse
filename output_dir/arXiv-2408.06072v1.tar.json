{
    "paper_id": "arXiv-2408.06072v1.tar",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-08-26T15:28:29.297483Z"
    },
    "title": "\n\nA Text-to-Video Diffusion Model with Expert Transformer",
    "authors": [
        {
            "first": "",
            "middle": [],
            "last": "Zhuoyi Yang* Jiayan Teng* Wendi Zheng Ming Ding Shiyu Huang Jiazheng Xu, Yuanming Yang Xiaohan Zhang Xiaotao Gu Guanyu Feng Da Yin Wenyi Hong, Weihan Wang Yean Cheng Yuxuan Zhang Ting Liu Bin Xu Yuxiao Dong Jie Tang, Zhipu AI, Tsinghua University, https://github.com/THUDM/CogVideo",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": "",
    "identifiers": {
        "arxiv_id": "arXiv-2408.06072v1.tar"
    },
    "abstract": "*equal contributions. Core contributors: Zhuoyi, Jiayan, Wendi, Ming, and Shiyu. {yangzy22,tengjy20}@mails.tsinghua.edu.cn, {yuxiaod,jietang}@tsinghua.edu.cn We introduce CogVideoX, a large diffusion transformer model capable of generating videos conditioned on text prompts. It applies a 3D VAE and a 3D transformer based on Expert Adaptive LayerNorm. Employing a progressive training technique, the model is able to generate coherent long-duration videos characterized by significant motion. Additionally, we propose a complete large-scale data processing pipeline, including various data cleaning strategies and video re-caption method, resulting in better generation quality and improved semantic alignment. Finally, CogVideoX achieves state-of-the-art performance across multiple machine metrics and human evaluations.",
    "latex_parse": {
        "paper_id": "arXiv-2408.06072v1.tar",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "*equal contributions. Core contributors: Zhuoyi, Jiayan, Wendi, Ming, and Shiyu. {yangzy22,tengjy20}@mails.tsinghua.edu.cn, {yuxiaod,jietang}@tsinghua.edu.cn We introduce CogVideoX, a large diffusion transformer model capable of generating videos conditioned on text prompts. It applies a 3D VAE and a 3D transformer based on Expert Adaptive LayerNorm. Employing a progressive training technique, the model is able to generate coherent long-duration videos characterized by significant motion. Additionally, we propose a complete large-scale data processing pipeline, including various data cleaning strategies and video re-caption method, resulting in better generation quality and improved semantic alignment. Finally, CogVideoX achieves state-of-the-art performance across multiple machine metrics and human evaluations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The rapid development of text-to-video models has been phenomenal, driven by both the Transformer architecture BIBREF0 and diffusion model BIBREF1 . Early attempts to pretrain and scale Transformers to generate videos from text have shown great promise, such as CogVideo BIBREF2 and Phenaki BIBREF3 . Meanwhile, diffusion models have recently made exciting advancements in multimodal generation, including video generation BIBREF4 , BIBREF5 . By using Transformers as the backbone of diffusion models, i.e., Diffusion Transformers (DiT) BIBREF6 , text-to-video generation has reached groundbreaking levels, as evidenced by the impressive Sora showcases BIBREF7 .",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 118,
                        "text": 33,
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 139,
                        "end": 146,
                        "text": 14,
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 271,
                        "end": 278,
                        "text": 16,
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 291,
                        "end": 298,
                        "text": 34,
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 423,
                        "end": 430,
                        "text": 29,
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 433,
                        "end": 440,
                        "text": 15,
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 537,
                        "end": 544,
                        "text": 24,
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 653,
                        "end": 660,
                        "text": 23,
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Despite these rapid advancements in DiTs, it remains technically unclear how to achieve long-term consistent video generation. Challenges such as efficiently modeling video data, effectively aligning videos with text semantics, as well as constructing the high-quality text-video pairs for model training have thus far been largely unaddressed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this work, we train and introduce CogVideoX, a set of large-scale diffusion transformer models designed for generating long-term, temporally consistent videos. We address the challenges mentioned above by developing a 3D variational Autoencoder (VAE), an expert Transformer, and a video data filtering and captioning pipeline, respectively. First, to efficiently consume video data, we design and train a 3D causal VAE that compresses the video along both spatial and temporal dimensions. Compared to unfolding a video into a one-dimensional sequence in the pixel space, this strategy helps significantly reduce the sequence length and associated training compute. Unlike previous video models BIBREF8 that often use a 2D VAE to encode each frame separately, the 3D VAE helps prevent flicker in the generated videos, that is, ensuring continuity among frames.",
                "cite_spans": [
                    {
                        "start": 697,
                        "end": 704,
                        "text": 6,
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Second, to improve the alignment between videos and texts, we propose an expert Transformer with expert adaptive LayerNorm to facilitate the fusion between the two modalities. To ensure the temporal consistency in video generation and capture large-scale motions, we propose to use 3D full attention to comprehensively model the video along both temporal and spatial dimensions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Third, as most video data available online lacks accurate textual descriptions, we develop a video captioning pipeline capable of accurately describing video content. This pipeline is used to generate new textual descriptions for all video data, which significantly enhances CogVideoX's ability to grasp precise semantic understanding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In addition, we adopt and design progressive training techniques, including mixed-duration training and resolution progressive training, to further enhance the generation performance and stability of CogVideoX. Furthermore, we propose Explicit Uniform Sampling, which stablizes the training loss curve and accelerates convergence by setting different timestep sampling intervals on each data parallel rank.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Both machine and human evaluations suggest that CogVideoX outperforms well-known public models. Figure FIGREF1 shows the performance of CogVideoX in different aspects.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 103,
                        "end": 110,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "CogVideoX is an ongoing attempt to advance text-to-video generation. To facilitate further developments, we open-source the model weight of part of CogVideoX and the 3D VAE, and we plan to release future and larger models as well. Now open-sourced CogVideoX is capable of generating 720 INLINEFORM0 480 videos of six seconds with eight frames per second. It can be publicly accessed from https://github.com/THUDM/CogVideo.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 287,
                        "end": 298,
                        "text": "\u00d7",
                        "latex": "\\times ",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>&#x000D7;</mi></mrow></math>",
                        "ref_id": "INLINEFORM0"
                    }
                ],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the section, we present the CogVideoX model. Figure FIGREF2 illustrates the overall architecture. Given a pair of video and text input, we design a 3D causal VAE to compress the video into the latent space, and the latents are then patchified and unfolded into a long sequence denoted as INLINEFORM0 . Simultaneously, we encode the textual input into text embeddings INLINEFORM1 using T5 BIBREF9 . Subsequently, INLINEFORM2 and INLINEFORM3 are concatenated along the sequence dimension. The concatenated embeddings are then fed into a stack of expert transformer blocks. Finally, the model output are unpatchified to restore the original latent shape, which is then decoded using a 3D causal VAE decoder to reconstruct the video. We illustrate the technical design of the 3D causal VAE and expert transfomer in detail.",
                "cite_spans": [
                    {
                        "start": 391,
                        "end": 398,
                        "text": 25,
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 55,
                        "end": 62,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [
                    {
                        "start": 291,
                        "end": 302,
                        "text": "z vision ",
                        "latex": "z_{\\text{vision}}",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><msub><mi>z</mi><mrow><mi>\\text</mi><mrow><mi>v</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mrow></msub></mrow></math>",
                        "ref_id": "INLINEFORM0"
                    },
                    {
                        "start": 370,
                        "end": 381,
                        "text": "z text ",
                        "latex": "z_{\\text{text}}",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><msub><mi>z</mi><mrow><mi>\\text</mi><mrow><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></mrow></msub></mrow></math>",
                        "ref_id": "INLINEFORM1"
                    },
                    {
                        "start": 415,
                        "end": 426,
                        "text": "z text ",
                        "latex": "z_{\\text{text}}",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><msub><mi>z</mi><mrow><mi>\\text</mi><mrow><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></mrow></msub></mrow></math>",
                        "ref_id": "INLINEFORM2"
                    },
                    {
                        "start": 431,
                        "end": 442,
                        "text": "z vision ",
                        "latex": "z_{\\text{vision}}",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><msub><mi>z</mi><mrow><mi>\\text</mi><mrow><mi>v</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mrow></msub></mrow></math>",
                        "ref_id": "INLINEFORM3"
                    }
                ],
                "section": "The CogVideoX Architecture",
                "sec_num": "2"
            },
            {
                "text": "Videos encompass not only spatial information but also substantial temporal information, usually resulting in orders of magnitude more data volumes than images. To tackle the computational challenge of modeling video data, we propose to implement a video compression module based on 3D Variational Autoencoders (3D VAEs) BIBREF10 . The idea is to incorporate three-dimentional convolutions to compress videos both spatially and temporally. This can help achieve a higher compression ratio with largely improved quality and continuity of video reconstruction when compared to previous image VAEs BIBREF11 , BIBREF12 .",
                "cite_spans": [
                    {
                        "start": 321,
                        "end": 329,
                        "text": 38,
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 595,
                        "end": 603,
                        "text": 26,
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 606,
                        "end": 614,
                        "text": 11,
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::3D Causal VAE",
                "sec_num": "2.1"
            },
            {
                "text": "Figure FIGREF4 (a) shows the structure of the proposed 3D VAE. It comprises an encoder, a decoder and a latent space regularizer. The Gaussian latent space is constrained by a Kullback-Leibler (KL) regularizer. The encoder and decoder consist of four symmetrically arranged stages, respectively performing 2 INLINEFORM0 downsampling and upsampling by the interleaving of ResNet block stacked stages. The first two rounds of downsampling and the last two upsampling involve both the spatial and temporal dimensions, while the last round only applies spatial sampling. This enables the 3D VAE to achieve a 4 INLINEFORM1 compression in the temporal dimension and an 8 INLINEFORM2 8 compression in the spatial dimension. In total, this achieves a 4 INLINEFORM3 8 INLINEFORM4 8 compression from pixels to the latents.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 14,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [
                    {
                        "start": 308,
                        "end": 319,
                        "text": "\u00d7",
                        "latex": "\\times ",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>&#x000D7;</mi></mrow></math>",
                        "ref_id": "INLINEFORM0"
                    },
                    {
                        "start": 606,
                        "end": 617,
                        "text": "\u00d7",
                        "latex": "\\times ",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>&#x000D7;</mi></mrow></math>",
                        "ref_id": "INLINEFORM1"
                    },
                    {
                        "start": 665,
                        "end": 676,
                        "text": "\u00d7",
                        "latex": "\\times ",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>&#x000D7;</mi></mrow></math>",
                        "ref_id": "INLINEFORM2"
                    },
                    {
                        "start": 745,
                        "end": 756,
                        "text": "\u00d7",
                        "latex": "\\times ",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>&#x000D7;</mi></mrow></math>",
                        "ref_id": "INLINEFORM3"
                    },
                    {
                        "start": 759,
                        "end": 770,
                        "text": "\u00d7",
                        "latex": "\\times ",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>&#x000D7;</mi></mrow></math>",
                        "ref_id": "INLINEFORM4"
                    }
                ],
                "section": "The CogVideoX Architecture::3D Causal VAE",
                "sec_num": "2.1"
            },
            {
                "text": "We adopt the temporally causal convolution BIBREF10 , which places all the paddings at the beginning of the convolution space, as shown in Figure FIGREF4 (b). This ensures the future information not to influence the present or past predictions. Given that processing videos with a large number of frames introduces excessive GPU memory usage, we apply context parallel at the temporal dimension for 3D convolution to distribute computation among multiple devices. As illustrated by Figure FIGREF4 (b), due to the causal nature of the convolution, each rank simply sends a segment of length INLINEFORM0 to the next rank, where INLINEFORM1 indicates the temporal kernel size. This results in relatively low communication overhead.",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 51,
                        "text": 38,
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 146,
                        "end": 153,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 489,
                        "end": 496,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [
                    {
                        "start": 590,
                        "end": 601,
                        "text": "k-1",
                        "latex": "k-1",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>k</mi><mo>&#x02212;</mo><mn>1</mn></mrow></math>",
                        "ref_id": "INLINEFORM0"
                    },
                    {
                        "start": 626,
                        "end": 637,
                        "text": "k",
                        "latex": "k",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>k</mi></mrow></math>",
                        "ref_id": "INLINEFORM1"
                    }
                ],
                "section": "The CogVideoX Architecture::3D Causal VAE",
                "sec_num": "2.1"
            },
            {
                "text": "During actual implementation, we first train a 3D VAE on lower resolutions and fewer frames to save computation. We observe that the encoding of larger resolution generalizes naturally, while extending the number of frames to be encoded does not work as seamlessly. Therefore, we conduct a two-stage training process by first training on short videos and finetuning by context parallel on long videos. Both stages of training utilize a weighted combination of the L2 loss, LPIPS BIBREF13 perceptual loss, and GAN loss from a 3D discriminator.",
                "cite_spans": [
                    {
                        "start": 479,
                        "end": 487,
                        "text": 42,
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::3D Causal VAE",
                "sec_num": "2.1"
            },
            {
                "text": "We introduce the design choices in Transformer for CogVideoX, including the patching, positional embedding, and attention strategies for handling the text-video data effectively and efficiently.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Expert Transformer",
                "sec_num": "2.2"
            },
            {
                "text": "The 3D causal VAE encodes a video latent vector of shape INLINEFORM0 , where INLINEFORM1 represents the number of frames, INLINEFORM2 and INLINEFORM3 represent the height and width of each frame, INLINEFORM4 represents the channel number, respectively. The video latents are then patchified along the spatial dimensions, generating sequence INLINEFORM5 of length INLINEFORM6 . Note that, we do not patchify along the temporal dimension in order to enable joint training of images and videos.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 57,
                        "end": 68,
                        "text": "T\u00d7H\u00d7W\u00d7C",
                        "latex": "T \\times H \\times W \\times C",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>T</mi><mi>&#x000D7;</mi><mi>H</mi><mi>&#x000D7;</mi><mi>W</mi><mi>&#x000D7;</mi><mi>C</mi></mrow></math>",
                        "ref_id": "INLINEFORM0"
                    },
                    {
                        "start": 77,
                        "end": 88,
                        "text": "T",
                        "latex": "T",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>T</mi></mrow></math>",
                        "ref_id": "INLINEFORM1"
                    },
                    {
                        "start": 122,
                        "end": 133,
                        "text": "H",
                        "latex": "H",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>H</mi></mrow></math>",
                        "ref_id": "INLINEFORM2"
                    },
                    {
                        "start": 138,
                        "end": 149,
                        "text": "W",
                        "latex": "W",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>W</mi></mrow></math>",
                        "ref_id": "INLINEFORM3"
                    },
                    {
                        "start": 196,
                        "end": 207,
                        "text": "C",
                        "latex": "C",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>C</mi></mrow></math>",
                        "ref_id": "INLINEFORM4"
                    },
                    {
                        "start": 341,
                        "end": 352,
                        "text": "z vision ",
                        "latex": "z_{\\text{vision}}",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><msub><mi>z</mi><mrow><mi>\\text</mi><mrow><mi>v</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mrow></msub></mrow></math>",
                        "ref_id": "INLINEFORM5"
                    },
                    {
                        "start": 363,
                        "end": 374,
                        "text": "T\u00b7H p\u00b7W p",
                        "latex": "T\\cdot \\frac{H}{p} \\cdot \\frac{W}{p}",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>T</mi><mo>&#x022C5;</mo><mfrac><mrow><mi>H</mi></mrow><mrow><mi>p</mi></mrow></mfrac><mo>&#x022C5;</mo><mfrac><mrow><mi>W</mi></mrow><mrow><mi>p</mi></mrow></mfrac></mrow></math>",
                        "ref_id": "INLINEFORM6"
                    }
                ],
                "section": "The CogVideoX Architecture::Expert Transformer::Patchify.",
                "sec_num": "2.2.0.1"
            },
            {
                "text": "Rotary Position Embedding (RoPE) BIBREF14 is a relative positional encoding that has been demonstrated to capture inter-token relationships effectively in LLMs, particularly excelling in modeling long sequences. To adapt to video data, we extend the original RoPE to 3D-RoPE. Each latent in the video tensor can be represented by a 3D coordinate INLINEFORM0 . We independently apply 1D-RoPE to each dimension of the coordinates, each occupying INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 of the hidden states' channel. The resulting encoding is then concatenated along the channel dimension to obtain the final 3D-RoPE encoding.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 41,
                        "text": 30,
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 346,
                        "end": 357,
                        "text": "(x,y,t)",
                        "latex": "(x, y, t)",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mo stretchy=\"false\">&#x00028;</mo><mi>x</mi><mi>,</mi><mi>y</mi><mi>,</mi><mi>t</mi><mo stretchy=\"false\">&#x00029;</mo></mrow></math>",
                        "ref_id": "INLINEFORM0"
                    },
                    {
                        "start": 444,
                        "end": 455,
                        "text": "3/8",
                        "latex": "3/8",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mn>3</mn><mo>&#x0002F;</mo><mn>8</mn></mrow></math>",
                        "ref_id": "INLINEFORM1"
                    },
                    {
                        "start": 458,
                        "end": 469,
                        "text": "3/8",
                        "latex": "3/8",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mn>3</mn><mo>&#x0002F;</mo><mn>8</mn></mrow></math>",
                        "ref_id": "INLINEFORM2"
                    },
                    {
                        "start": 476,
                        "end": 487,
                        "text": "2/8",
                        "latex": "2/8",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mn>2</mn><mo>&#x0002F;</mo><mn>8</mn></mrow></math>",
                        "ref_id": "INLINEFORM3"
                    }
                ],
                "section": "The CogVideoX Architecture::Expert Transformer::3D-RoPE.",
                "sec_num": "2.2.0.2"
            },
            {
                "text": "We empirically examine the use of RoPE. Figure FIGREF8 (a)shows the comparison between 3D RoPE and sinusoidal absolute position encoding. We can observe that the loss curve using 3D RoPE converges significantly faster than that with sinusoidal encoding. We further compare the use of 3D RoPE alone against the combination of 3D RoPE and learnable absolute position embedding. Figure FIGREF8 (b) indicates that the loss curves of both methods converge almost identically. Therefore, we choose to use 3D RoPE alone for simplicity.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 47,
                        "end": 54,
                        "text": "4",
                        "ref_id": "FIGREF8"
                    },
                    {
                        "start": 383,
                        "end": 390,
                        "text": "4",
                        "ref_id": "FIGREF8"
                    }
                ],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Expert Transformer::3D-RoPE.",
                "sec_num": "2.2.0.2"
            },
            {
                "text": "We concatenate the embeddings of both text and video at the input stage to better align visual and semantic information. However, the feature spaces of these two modalities differ significantly, and their embeddings may even have different numerical scales. To better process them within the same sequence, we employ the Expert Adaptive Layernorm to handle each modality independently. As shown in Figure FIGREF2 , following DiT BIBREF6 , we use the timestep INLINEFORM0 of the diffusion process as the input to the modulation module. Then, the Vision Expert Adaptive Layernorm (Vison Expert AdaLN) and Text Expert Adaptive Layernorm (Text Expert AdaLN) apply this modulation mechanism to the vision hidden states and text hidden states, respectively. This strategy promotes the alignment of feature spaces across two modalities while minimizing additional parameters.",
                "cite_spans": [
                    {
                        "start": 429,
                        "end": 436,
                        "text": 24,
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 405,
                        "end": 412,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [
                    {
                        "start": 459,
                        "end": 470,
                        "text": "t",
                        "latex": "t",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>t</mi></mrow></math>",
                        "ref_id": "INLINEFORM0"
                    }
                ],
                "section": "The CogVideoX Architecture::Expert Transformer::Expert Transformer Block.",
                "sec_num": "2.2.0.3"
            },
            {
                "text": "To verify the adoption of Expert Adaptive Layernorm, we experiment with different ways of incorporating experts: expert LayerNorm and MLP, and expert Layernorm only. Our experiments find that adding expert MLP does not effectively accelerate the model's convergence (Cf. Figure FIGREF8 (c)). To reduce the model parameters, we only choose to use Expert Adaptive Layernorm.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 278,
                        "end": 285,
                        "text": "4",
                        "ref_id": "FIGREF8"
                    }
                ],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Expert Transformer::Expert Transformer Block.",
                "sec_num": "2.2.0.3"
            },
            {
                "text": "Previous works BIBREF4 , BIBREF15 often employ separated spatial and temporal attention to reduce computational complexity and facilitate fine-tuning from text-to-image models. However, as illustrated in Figure FIGREF14 , this separated attention approach requires extensive implicit transmission of visual information, significantly increasing the learning complexity and making it challenging to maintain the consistency of large-movement objects. Considering the great success of long-context training in LLMs BIBREF16 , BIBREF17 , BIBREF18 and the efficiency of FlashAttention BIBREF19 , we propose a 3D text-video hybrid attention mechanism. This mechanism not only achieves better results but can also be easily adapted to various parallel acceleration methods.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 22,
                        "text": 29,
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 25,
                        "end": 33,
                        "text": 13,
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 513,
                        "end": 521,
                        "text": 2,
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 524,
                        "end": 532,
                        "text": 3,
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 535,
                        "end": 543,
                        "text": 37,
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 581,
                        "end": 589,
                        "text": 9,
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 211,
                        "end": 219,
                        "text": "5",
                        "ref_id": "FIGREF14"
                    }
                ],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Expert Transformer::3D Full Attention.",
                "sec_num": "2.2.0.4"
            },
            {
                "text": "We construct a collection of relatively high-quality video clips with text descriptions through video filters and recaption models. After filtering, approximately 35M single-shot clips remain, with each clip averaging about 6 seconds.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data",
                "sec_num": "2.3"
            },
            {
                "text": "Video generation models need to learn the dynamic information of the world, but unfiltered video data is of highly noisy distribution, primarily due to two reasons: First, videos are human-created, and artificial editing may distort the authentic dynamic information; Second, the quality of videos can significantly drop due to issues during filming, such as camera shakes and substandard equipment.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "In addition to the intrinsic quality of the videos, we also consider how well the video data supports model training. Videos with minimal dynamic information or lacking connectivity in dynamic aspects are considered detrimental. Consequently, we have developed a set of negative labels, which include:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "1.  Editing : Videos that have undergone obvious artificial processing, such as re-editing and special effects, causing degradation of the visual integrity. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "2.  Lack of Motion Connectivity : Video segments with image transitions lacking motion connectivity, commonly seen in videos artificially spliced or edited from images. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "3.  Low Quality : Poorly shot videos with unclear visuals or excessive camera shake. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "4.  Lecture Type : Videos focusing primarily on a person continuously talking with minimal effective motion, such as educational content, lectures, and live-streamed discussions. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "5.  Text Dominated : Videos containing a substantial amount of visible text or primarily focusing on textual content. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "6.  Noisy Screenshots : Noisy videos recorded from phone or computer screens. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "We sample 20,000 video data samples and label the presence of negative tags in each of them. By using these annotations, we train several filters based on video-llama BIBREF20 to screen out low-quality video data.",
                "cite_spans": [
                    {
                        "start": 167,
                        "end": 175,
                        "text": 41,
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "In addition, we calculate the optical flow scores and image aesthetic scores of all training videos and dynamically adjust the threshold ranges during training to ensure the fluency and aesthetic quality of the generated videos.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Filtering.",
                "sec_num": "2.3.0.5"
            },
            {
                "text": "Typically, most video data does not come with corresponding descriptive text, so it is necessary to convert the video data into textual descriptions to provide the essential training data for text-to-video models. Currently, there are some video caption datasets available, such as Panda70M BIBREF21 , COCO Caption BIBREF22 , and WebVid BIBREF23 . However, the captions in these datasets are usually very short and fail to describe the video's content comprehensively.",
                "cite_spans": [
                    {
                        "start": 291,
                        "end": 299,
                        "text": 8,
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 315,
                        "end": 323,
                        "text": 21,
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 337,
                        "end": 345,
                        "text": 4,
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Caption.",
                "sec_num": "2.3.0.6"
            },
            {
                "text": "To generate high-quality video caption data, we establish a Dense Video Caption Data Generation pipeline, as detailed in Figure FIGREF25 . The idea is to generate video captions from image captions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 128,
                        "end": 136,
                        "text": "6",
                        "ref_id": "FIGREF25"
                    }
                ],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Caption.",
                "sec_num": "2.3.0.6"
            },
            {
                "text": "First, we use the Panda70M video captioning model BIBREF21 to generate short captions for the videos. Then, we employ the image recaptioning model CogVLM BIBREF24 used in Stable Diffusion 3 BIBREF25 and CogView3 BIBREF26 to create dense image captions for each of the frames within a video. Subsequently, we use GPT-4 to summarize all the image captions to produce the final video caption. To accelerate the generation from image captions to video captions, we fine-tune a Llama2 model BIBREF27 using the summary data generated by GPT-4 BIBREF28 , enabling large-scale video caption data generation. Additional details regarding the video caption data generation process can be found in Appendix .",
                "cite_spans": [
                    {
                        "start": 50,
                        "end": 58,
                        "text": 8,
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 154,
                        "end": 162,
                        "text": 35,
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 190,
                        "end": 198,
                        "text": 12,
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 212,
                        "end": 220,
                        "text": 43,
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 486,
                        "end": 494,
                        "text": 32,
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 537,
                        "end": 545,
                        "text": 1,
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Caption.",
                "sec_num": "2.3.0.6"
            },
            {
                "text": "The pipeline above generates the caption data that is used to trained the CogVideoX model introduced in this report. To further accelerate video recaptioning, we also fine-tune an end-to-end video understanding model CogVLM2-Caption, based on the CogVLM2-Video FOOTREF26 and Llama3 BIBREF16 , by using the dense caption data generated from the aforementioned pipeline. The video caption data generated by CogVLM2-Caption is used to train the next generation of CogVideoX. Examples of video captions generated by this end-to-end CogVLM2-Caption model are shown in Appendix . In Appendix , we also present some examples of video generation where a video is first input into CogVLM2-Caption to generate captions, which are then used as input for CogVideoX to generate new videos, effectively achieving video-to-video generation.",
                "cite_spans": [
                    {
                        "start": 282,
                        "end": 290,
                        "text": 2,
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 261,
                        "end": 270,
                        "text": "1",
                        "ref_id": "FOOTREF26"
                    }
                ],
                "eq_spans": [],
                "section": "The CogVideoX Architecture::Data::Video Caption.",
                "sec_num": "2.3.0.6"
            },
            {
                "text": "We mix images and videos during training, treating each image as a single-frame video. Additionally, we employ progressive training from the resolution perspective. For the diffusion setting, we adopt v-prediction BIBREF29 and zero SNR BIBREF30 , following the noise schedule used in LDM BIBREF11 . During diffusion training for timestep sampling, we also employ an explicit uniform timestep sampling method, which benefits training stability.",
                "cite_spans": [
                    {
                        "start": 214,
                        "end": 222,
                        "text": 28,
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 236,
                        "end": 244,
                        "text": 20,
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 288,
                        "end": 296,
                        "text": 26,
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training CogVideoX",
                "sec_num": "3"
            },
            {
                "text": "Previous video training methods often involve joint training of images and videos with fixed number of frames BIBREF4 , BIBREF8 . However, this approach usually leads to two issues: First, there is a significant gap between the two input types using bidirectional attention, with images having one frame while videos having dozens of frames. We observe that models trained this way tend to diverge into two generative modes based on the token count and not to have good generalizations. Second, to train with a fixed duration, we have to discard short videos and truncate long videos, which prevents full utilization of the videos of varying number of frames.",
                "cite_spans": [
                    {
                        "start": 110,
                        "end": 117,
                        "text": 29,
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 120,
                        "end": 127,
                        "text": 6,
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training CogVideoX::Frame Pack",
                "sec_num": "3.1"
            },
            {
                "text": "To address these issues, we choose mixed-duration training, which means training videos of different lengths together. However, inconsistent data shapes within the batch make training difficult. Inspired by Patch'n Pack BIBREF31 , we place videos of different lengths into the same batch to ensure consistent shapes within each batch, a method we refer to as Frame Pack . The process is illustrated in Figure FIGREF27 .",
                "cite_spans": [
                    {
                        "start": 220,
                        "end": 228,
                        "text": 10,
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 409,
                        "end": 417,
                        "text": "7",
                        "ref_id": "FIGREF27"
                    }
                ],
                "eq_spans": [],
                "section": "Training CogVideoX::Frame Pack",
                "sec_num": "3.1"
            },
            {
                "text": "The training pipeline of CogVideoX is divided into three stages: low-resolution training, high-resolution training, and high-quality video fine-tuning. Similar to images, videos from the Internet usually include a significant amount of low-resolution ones. Progressive training can effectively utilize videos of various resolutions. Moreover, training at low resolution initially can equip the model with coarse-grained modeling capabilities, followed by high-resolution training to enhance its ability to capture fine details. Compared to direct high-resolution training, staged training can also help reduce the overall training time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training CogVideoX::Resolution Progressive Training",
                "sec_num": "3.2"
            },
            {
                "text": "When adapting low-resolution position encoding to high-resolution, we consider two different methods: interpolation and extrapolation. We show the effects of two methods in Figure FIGREF30 . Interpolation tens to preserve global information more effectively, whereas the extrapolation better retains local details. Given that RoPE is a relative position encoding, We chose the extrapolation to maintain the relative position between pixels.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 180,
                        "end": 188,
                        "text": "8",
                        "ref_id": "FIGREF30"
                    }
                ],
                "eq_spans": [],
                "section": "Training CogVideoX::Resolution Progressive Training::Extrapolation of Position Code.",
                "sec_num": "3.2.0.7"
            },
            {
                "text": "Since the filtered pre-training data still contains a certain proportion of dirty data, such as subtitles, watermarks, and low-bitrate videos, we selected a subset of higher quality video data, accounting for 20% of the total dataset, for fine-tuning in the final stage. This step effectively removed generated subtitles and watermarks and slightly improved the visual quality. However, we also observed a slight degradation in the model's semantic ability.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training CogVideoX::Resolution Progressive Training::High-Quality Fine-Tuning.",
                "sec_num": "3.2.0.8"
            },
            {
                "text": " BIBREF1 defines the training objective of diffusion as",
                "cite_spans": [
                    {
                        "start": 1,
                        "end": 8,
                        "text": 14,
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training CogVideoX::Explicit Uniform Sampling",
                "sec_num": "3.3"
            },
            {
                "text": " DISPLAYFORM0 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 1,
                        "end": 13,
                        "text": "L  simple  (\u03b8):=\ud835\udc04 t,x 0 ,\u03f5 \u03f5-\u03f5 \u03b8 (\u03b1 \u00af t x 0 +1-\u03b1 \u00af t \u03f5,t) 2 ,",
                        "latex": "~\nL_\\mathrm {simple}(\\theta ) := \\mathbf {E}_{t, x_0, \\epsilon }{ \\left\\Vert  \\epsilon - \\epsilon _\\theta (\\sqrt{\\bar{\\alpha }_t} x_0 + \\sqrt{1-\\bar{\\alpha }_t}\\epsilon , t) \\right\\Vert ^2},\n",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>~</mi><msub><mi>L</mi><mi>\\mathrm</mi></msub><mrow><mi>s</mi><mi>i</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi></mrow><mo stretchy=\"false\">&#x00028;</mo><mi>&#x003B8;</mi><mo stretchy=\"false\">&#x00029;</mo><mi>:</mi><mo>&#x0003D;</mo><msub><mrow><mi>E</mi></mrow><mrow><mi>t</mi><mi>,</mi><msub><mi>x</mi><mn>0</mn></msub><mi>,</mi><mi>&#x003F5;</mi></mrow></msub><mrow><msup><mrow><mo stretchy=\"true\" fence=\"true\" form=\"prefix\">&#x02016;</mo><mrow><mi>&#x003F5;</mi><mo>&#x02212;</mo><msub><mi>&#x003F5;</mi><mi>&#x003B8;</mi></msub><mo stretchy=\"false\">&#x00028;</mo><msqrt><mrow><mover><mi>_</mi><mo stretchy=\"true\">&#x000AF;</mo></mover><mrow><mi>&#x003B1;</mi></mrow><mi>t</mi></mrow></msqrt><msub><mi>x</mi><mn>0</mn></msub><mo>&#x0002B;</mo><msqrt><mrow><mn>1</mn><mo>&#x02212;</mo><mover><mi>_</mi><mo stretchy=\"true\">&#x000AF;</mo></mover><mrow><mi>&#x003B1;</mi></mrow><mi>t</mi></mrow></msqrt><mi>&#x003F5;</mi><mi>,</mi><mi>t</mi><mo stretchy=\"false\">&#x00029;</mo></mrow><mo stretchy=\"true\" fence=\"true\" form=\"postfix\">&#x02016;</mo></mrow><mn>2</mn></msup></mrow><mi>,</mi></mrow></math>",
                        "ref_id": "DISPLAYFORM0"
                    }
                ],
                "section": "Training CogVideoX::Explicit Uniform Sampling",
                "sec_num": "3.3"
            },
            {
                "text": "where INLINEFORM0 is uniformly distributed between 1 and T. The common practice is for each rank in the data parallel group to uniformly sample a value between 1 and INLINEFORM1 , which is in theory equivalent to Equation EQREF34 . However, in practice, the results obtained from such random sampling are often not sufficiently uniform, and since the magnitude of the diffusion loss is related to the timesteps, this can lead to significant fluctuations in the loss. Thus, we propose to use Explicit Uniform Sampling to divide the range from 1 to INLINEFORM2 into INLINEFORM3 intervals, where INLINEFORM4 is the number of ranks. Each rank then uniformly samples within its respective interval. This method ensures a more uniform distribution of timesteps. As shown in Figure FIGREF8 (d), the loss curve from training with Explicit Uniform Sampling is noticeably more stable.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 775,
                        "end": 782,
                        "text": "4",
                        "ref_id": "FIGREF8"
                    },
                    {
                        "start": 222,
                        "end": 229,
                        "text": "1",
                        "ref_id": "EQREF34"
                    }
                ],
                "eq_spans": [
                    {
                        "start": 6,
                        "end": 17,
                        "text": "t",
                        "latex": "t",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>t</mi></mrow></math>",
                        "ref_id": "INLINEFORM0"
                    },
                    {
                        "start": 166,
                        "end": 177,
                        "text": "T",
                        "latex": "T",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>T</mi></mrow></math>",
                        "ref_id": "INLINEFORM1"
                    },
                    {
                        "start": 547,
                        "end": 558,
                        "text": "T",
                        "latex": "T",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>T</mi></mrow></math>",
                        "ref_id": "INLINEFORM2"
                    },
                    {
                        "start": 564,
                        "end": 575,
                        "text": "n",
                        "latex": "n",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>n</mi></mrow></math>",
                        "ref_id": "INLINEFORM3"
                    },
                    {
                        "start": 593,
                        "end": 604,
                        "text": "n",
                        "latex": "n",
                        "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>n</mi></mrow></math>",
                        "ref_id": "INLINEFORM4"
                    }
                ],
                "section": "Training CogVideoX::Explicit Uniform Sampling",
                "sec_num": "3.3"
            },
            {
                "text": "In addition, we compare the loss at each diffusion timestep alone between the two methods for a more precise comparison. We find that after using explicit uniform sampling, the loss at all timesteps decreased faster, indicating that this method can accelerate loss convergence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training CogVideoX::Explicit Uniform Sampling",
                "sec_num": "3.3"
            },
            {
                "text": "We conducted ablation studies on some of the designs mentioned in Section SECREF2 to verify their effectiveness.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 74,
                        "end": 81,
                        "text": "2",
                        "ref_id": "SECREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Training CogVideoX::Explicit Uniform Sampling",
                "sec_num": "3.3"
            },
            {
                "text": "First, we compared 3D RoPE with sinusoidal absolute position encoding. As shown in Figure , the loss curve using 3D RoPE converges significantly faster than that with sinusoidal encoding. Then we compared the use of 3D RoPE alone with the combination of 3D RoPE and learnable absolute position embedding. As shown in Figure , the loss curves of both methods converge almost identically. For simplicity, we chose to use 3D RoPE alone.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training CogVideoX::Position Embedding",
                "sec_num": "3.4"
            },
            {
                "text": "We experimented with different ways of incorporating experts: expert LayerNorm and MLP, and expert Layernorm only. Our experiments found that adding expert MLP does not effectively accelerate the model's convergence (Figure ). To reduce the model parameters, we only chose to use expert adaptive Layernorm.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training CogVideoX::Expert Adaptive Layernorm",
                "sec_num": "3.5"
            },
            {
                "text": "In this section, we present the performance of CogVideoX through two primary methods: automated metric evaluation and human assessment . We train the CogVideoX models with different parameter sizes. We show results for 2B and 5B for now, larger models are still in training.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Evaluation",
                "sec_num": "4"
            },
            {
                "text": "To facilitate the development of text-to-video generation, we open-source the model weight at https://github.com/THUDM/CogVideo.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Evaluation",
                "sec_num": "4"
            },
            {
                "text": "We choose openly-accessible top-performing text-to-video models as baselines, including T2V-Turbo BIBREF32 , AnimateDiff BIBREF15 , VideoCrafter2 BIBREF33 , OpenSora BIBREF34 , Show-1 BIBREF35 , Gen-2 BIBREF36 , Pika BIBREF37 , and LaVie-2 BIBREF38 .",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 106,
                        "text": 18,
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 121,
                        "end": 129,
                        "text": 13,
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 146,
                        "end": 154,
                        "text": 7,
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 166,
                        "end": 174,
                        "text": 44,
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 184,
                        "end": 192,
                        "text": 40,
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 201,
                        "end": 209,
                        "text": 27,
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 217,
                        "end": 225,
                        "text": 0,
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 240,
                        "end": 248,
                        "text": 36,
                        "ref_id": "BIBREF38"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Evaluation::Automated Metric Evaluation::Baselines.",
                "sec_num": "4.1.0.9"
            },
            {
                "text": "To evaluate the text-to-video generation, we employed several metrics from VBench BIBREF39 : Human Action , Scene , Dynamic Degree , Multiple Objects , and Appearance Style . VBench is a suite of tools designed to automatically assess the quality of generated videos. We have selected certain metrics from VBench, excluding others that do not align with our evaluation needs. For example, the color metric, intended to measure the presence of objects corresponding to specific colors across frames in the generated video, assesses the model's quality by calculating the probability. However, this metric may mislead video generation models that exhibit greater variation, thus it is not to include it in our evaluation.",
                "cite_spans": [
                    {
                        "start": 82,
                        "end": 90,
                        "text": 17,
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Evaluation::Automated Metric Evaluation::Evaluation Metrics.",
                "sec_num": "4.1.0.10"
            },
            {
                "text": "For longer-generated videos, some models might produce videos with minimal changes between frames to obtain higher scores, but these videos lack rich content. Therefore, a metric for evaluating the dynamism of the video becomes more important. To address this, we employ two video evaluation tools: Dynamic Quality from Devil BIBREF40 and GPT4o-MTScore from ChronoMagic BIBREF41 , which focus more on the dynamic characteristics of videos. Dynamic Quality is defined by the integration of various quality metrics with dynamic scores, mitigating biases arising from negative correlations between video dynamics and video quality. ChronoMagic, for instance, introduces GPT4o-MTScore, a metric designed to measure the metamorphic amplitude of time-lapse videos, such as those depicting physical, biological, and meteorological changes. This metric using GPT-4o BIBREF42 to score the degree of change, providing a fine-grained assessment of video dynamism.",
                "cite_spans": [
                    {
                        "start": 326,
                        "end": 334,
                        "text": 19,
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 370,
                        "end": 378,
                        "text": 39,
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 858,
                        "end": 866,
                        "text": 22,
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Evaluation::Automated Metric Evaluation::Evaluation Metrics.",
                "sec_num": "4.1.0.10"
            },
            {
                "text": "Table TABREF43 provides the performance comparison of CogVideoX and other models. CogVideoX achieves the best performance in five out of the seven metrics and shows competitive results in the remaining two metrics. These results demonstrate that the model not only excels in video generation quality but also outperforms previous models in handling various complex dynamic scenes. In addition, Figure FIGREF1 presents a radar chart that visually illustrates the performance advantages of CogVideoX.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 401,
                        "end": 408,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 6,
                        "end": 14,
                        "text": "1",
                        "ref_id": "TABREF43"
                    }
                ],
                "eq_spans": [],
                "section": "Empirical Evaluation::Automated Metric Evaluation::Results.",
                "sec_num": "4.1.0.11"
            },
            {
                "text": "In addition to automated scoring mechanisms, a comparative analysis between the Kling BIBREF43 and CogVideoX is conducted with manual evaluation. One hundred meticulously crafted prompts are used for human evaluators, characterized by their broad distribution, clear articulation, and well-defined conceptual scope. We randomize videos for blind evalution. A panel of evaluators is instructed to assign scores for each detail on a scale from zero to one, with the overall total score rated on a scale from 0 to 5, where higher scores reflect better video quality. To better complement automated evaluation, human evaluation emphasizes the instruction-following capability: the total score cannot exceed 2 if the generated video fails to follow the instruction.",
                "cite_spans": [
                    {
                        "start": 86,
                        "end": 94,
                        "text": 31,
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Evaluation::Human Evaluation",
                "sec_num": "4.2"
            },
            {
                "text": "The results shown in Table TABREF45 indicate that CogVideoX wins the human preference over Kling across all aspects. More details about human evaluation are shown in Appendix .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 27,
                        "end": 35,
                        "text": "2",
                        "ref_id": "TABREF45"
                    }
                ],
                "eq_spans": [],
                "section": "Empirical Evaluation::Human Evaluation",
                "sec_num": "4.2"
            },
            {
                "text": "We would like to thank all the data annotators, infra operating staffs, collaborators, and partners as well as everyone at Zhipu AI and Tsinghua University not explicitly mentioned in the report who have provided support, feedback, and contributed to CogVideoX. We would also like to greatly thank BiliBili for data support.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empirical Evaluation::Human Evaluation::Acknowledgments",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF37": {
                "ref_id": "BIBREF37",
                "title": "Pika beta",
                "authors": [],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 0,
                "urls": [
                    "https://pika.art/home"
                ],
                "raw_text": "Pika beta. 2023. URL https://pika.art/home.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "BIBREF28",
                "title": "Shyamal Anadkat, et al. Gpt-4 technical report",
                "authors": [
                    {
                        "first": "Josh",
                        "middle": [],
                        "last": "Achiam",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Adler",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Lama",
                        "middle": [],
                        "last": "Ahmad",
                        "suffix": ""
                    },
                    {
                        "first": "Ilge",
                        "middle": [],
                        "last": "Akkaya",
                        "suffix": ""
                    },
                    {
                        "first": "Florencia",
                        "middle": [],
                        "last": "Leoni Aleman",
                        "suffix": ""
                    },
                    {
                        "first": "Diogo",
                        "middle": [],
                        "last": "Almeida",
                        "suffix": ""
                    },
                    {
                        "first": "Janko",
                        "middle": [],
                        "last": "Altenschmidt",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Altman",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2303.08774"
                    ]
                },
                "num": 1,
                "urls": [],
                "raw_text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "BIBREF16",
                "title": "Llama 3 model card",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ai@meta",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 2,
                "urls": [
                    "https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md"
                ],
                "raw_text": "AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "BIBREF17",
                "title": "A recipe for long context alignment of large language models",
                "authors": [
                    {
                        "first": "Yushi",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Lv",
                        "suffix": ""
                    },
                    {
                        "first": "Jiajie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuze",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxiao",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Juanzi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Longalign",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2401.18058"
                    ]
                },
                "num": 3,
                "urls": [],
                "raw_text": "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. Longalign: A recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058, 2024.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "BIBREF23",
                "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
                "authors": [
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Bain",
                        "suffix": ""
                    },
                    {
                        "first": "Arsha",
                        "middle": [],
                        "last": "Nagrani",
                        "suffix": ""
                    },
                    {
                        "first": "G\u00fcl",
                        "middle": [],
                        "last": "Varol",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the IEEE/CVF international conference on computer vision",
                "volume": "",
                "issue": "",
                "pages": "1728--1738",
                "other_ids": {},
                "num": 4,
                "urls": [],
                "raw_text": "Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1728\u20131738, 2021.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "BIBREF44",
                "title": "Improving image generation with better captions",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Betker",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Goh",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Jing",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Brooks",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Linjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Long",
                        "middle": [],
                        "last": "Ouyang",
                        "suffix": ""
                    },
                    {
                        "first": "Juntang",
                        "middle": [],
                        "last": "Zhuang",
                        "suffix": ""
                    },
                    {
                        "first": "Joyce",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Yufei",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Computer Science",
                "volume": "2",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": 5,
                "urls": [],
                "raw_text": "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "BIBREF8",
                "title": "Stable video diffusion: Scaling latent video diffusion models to large datasets",
                "authors": [
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Blattmann",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Dockhorn",
                        "suffix": ""
                    },
                    {
                        "first": "Sumith",
                        "middle": [],
                        "last": "Kulal",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Mendelevitch",
                        "suffix": ""
                    },
                    {
                        "first": "Maciej",
                        "middle": [],
                        "last": "Kilian",
                        "suffix": ""
                    },
                    {
                        "first": "Dominik",
                        "middle": [],
                        "last": "Lorenz",
                        "suffix": ""
                    },
                    {
                        "first": "Yam",
                        "middle": [],
                        "last": "Levi",
                        "suffix": ""
                    },
                    {
                        "first": "Zion",
                        "middle": [],
                        "last": "English",
                        "suffix": ""
                    },
                    {
                        "first": "Vikram",
                        "middle": [],
                        "last": "Voleti",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Letts",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2311.15127"
                    ]
                },
                "num": 6,
                "urls": [],
                "raw_text": "Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "BIBREF33",
                "title": "Videocrafter2: Overcoming data limitations for high-quality video diffusion models",
                "authors": [
                    {
                        "first": "Haoxin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "Cun",
                        "suffix": ""
                    },
                    {
                        "first": "Menghan",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Xintao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Chao",
                        "middle": [],
                        "last": "Weng",
                        "suffix": ""
                    },
                    {
                        "first": "Ying",
                        "middle": [],
                        "last": "Shan",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 7,
                "urls": [],
                "raw_text": "Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024a.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "BIBREF21",
                "title": "Panda-70m: Captioning 70m videos with multiple cross-modality teachers",
                "authors": [
                    {
                        "first": "Aliaksandr",
                        "middle": [],
                        "last": "Tsai-Shien Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Willi",
                        "middle": [],
                        "last": "Siarohin",
                        "suffix": ""
                    },
                    {
                        "first": "Ekaterina",
                        "middle": [],
                        "last": "Menapace",
                        "suffix": ""
                    },
                    {
                        "first": "Hsiang-Wei",
                        "middle": [],
                        "last": "Deyneka",
                        "suffix": ""
                    },
                    {
                        "first": "Byung",
                        "middle": [],
                        "last": "Chao",
                        "suffix": ""
                    },
                    {
                        "first": "Yuwei",
                        "middle": [],
                        "last": "Eun Jeon",
                        "suffix": ""
                    },
                    {
                        "first": "Hsin-Ying",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Hsuan",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "13320--13331",
                "other_ids": {},
                "num": 8,
                "urls": [],
                "raw_text": "Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13320\u201313331, 2024b.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "BIBREF19",
                "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness",
                "authors": [
                    {
                        "first": "Tri",
                        "middle": [],
                        "last": "Dao",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Ermon",
                        "suffix": ""
                    },
                    {
                        "first": "Atri",
                        "middle": [],
                        "last": "Rudra",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "R\u00e9",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "35",
                "issue": "",
                "pages": "16344--16359",
                "other_ids": {},
                "num": 9,
                "urls": [],
                "raw_text": "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "BIBREF31",
                "title": "Patch n\u2019pack: Navit, a vision transformer for any aspect ratio and resolution",
                "authors": [
                    {
                        "first": "Mostafa",
                        "middle": [],
                        "last": "Dehghani",
                        "suffix": ""
                    },
                    {
                        "first": "Basil",
                        "middle": [],
                        "last": "Mustafa",
                        "suffix": ""
                    },
                    {
                        "first": "Josip",
                        "middle": [],
                        "last": "Djolonga",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Heek",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Minderer",
                        "suffix": ""
                    },
                    {
                        "first": "Mathilde",
                        "middle": [],
                        "last": "Caron",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Steiner",
                        "suffix": ""
                    },
                    {
                        "first": "Joan",
                        "middle": [],
                        "last": "Puigcerver",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Geirhos",
                        "suffix": ""
                    },
                    {
                        "first": "Ibrahim",
                        "middle": [
                            "M"
                        ],
                        "last": "Alabdulmohsin",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "36",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 10,
                "urls": [],
                "raw_text": "Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch n\u2019pack: Navit, a vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "BIBREF12",
                "title": "Taming transformers for high-resolution image synthesis",
                "authors": [
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Esser",
                        "suffix": ""
                    },
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Rombach",
                        "suffix": ""
                    },
                    {
                        "first": "Bjorn",
                        "middle": [],
                        "last": "Ommer",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "12873--12883",
                "other_ids": {},
                "num": 11,
                "urls": [],
                "raw_text": "Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873\u201312883, 2021.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "BIBREF25",
                "title": "Scaling rectified flow transformers for high-resolution image synthesis",
                "authors": [
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Esser",
                        "suffix": ""
                    },
                    {
                        "first": "Sumith",
                        "middle": [],
                        "last": "Kulal",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Blattmann",
                        "suffix": ""
                    },
                    {
                        "first": "Rahim",
                        "middle": [],
                        "last": "Entezari",
                        "suffix": ""
                    },
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "M\u00fcller",
                        "suffix": ""
                    },
                    {
                        "first": "Harry",
                        "middle": [],
                        "last": "Saini",
                        "suffix": ""
                    },
                    {
                        "first": "Yam",
                        "middle": [],
                        "last": "Levi",
                        "suffix": ""
                    },
                    {
                        "first": "Dominik",
                        "middle": [],
                        "last": "Lorenz",
                        "suffix": ""
                    },
                    {
                        "first": "Axel",
                        "middle": [],
                        "last": "Sauer",
                        "suffix": ""
                    },
                    {
                        "first": "Frederic",
                        "middle": [],
                        "last": "Boesel",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Forty-first International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 12,
                "urls": [],
                "raw_text": "Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "BIBREF15",
                "title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning",
                "authors": [
                    {
                        "first": "Yuwei",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Ceyuan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Anyi",
                        "middle": [],
                        "last": "Rao",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengyang",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Yaohui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Qiao",
                        "suffix": ""
                    },
                    {
                        "first": "Maneesh",
                        "middle": [],
                        "last": "Agrawala",
                        "suffix": ""
                    },
                    {
                        "first": "Dahua",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2307.04725"
                    ]
                },
                "num": 13,
                "urls": [],
                "raw_text": "Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "BIBREF1",
                "title": "Denoising diffusion probabilistic models",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "Ajay",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "Pieter",
                        "middle": [],
                        "last": "Abbeel",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in neural information processing systems",
                "volume": "33",
                "issue": "",
                "pages": "6840--6851",
                "other_ids": {},
                "num": 14,
                "urls": [],
                "raw_text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "BIBREF5",
                "title": "Imagen video: High definition video generation with diffusion models",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Chan",
                        "suffix": ""
                    },
                    {
                        "first": "Chitwan",
                        "middle": [],
                        "last": "Saharia",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [],
                        "last": "Whang",
                        "suffix": ""
                    },
                    {
                        "first": "Ruiqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Alexey",
                        "middle": [],
                        "last": "Gritsenko",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Poole",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "J"
                        ],
                        "last": "Norouzi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Fleet",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2210.02303"
                    ]
                },
                "num": 15,
                "urls": [],
                "raw_text": "Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "BIBREF2",
                "title": "Cogvideo: Large-scale pretraining for text-to-video generation via transformers",
                "authors": [
                    {
                        "first": "Wenyi",
                        "middle": [],
                        "last": "Hong",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Wendi",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Xinghan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2205.15868"
                    ]
                },
                "num": 16,
                "urls": [],
                "raw_text": "Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "BIBREF39",
                "title": "VBench: Comprehensive benchmark suite for video generative models",
                "authors": [
                    {
                        "first": "Ziqi",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Yinan",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jiashuo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Fan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Chenyang",
                        "middle": [],
                        "last": "Si",
                        "suffix": ""
                    },
                    {
                        "first": "Yuming",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanhan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Tianxing",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Qingyang",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Nattapol",
                        "middle": [],
                        "last": "Chanpaisit",
                        "suffix": ""
                    },
                    {
                        "first": "Yaohui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyuan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Limin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Dahua",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Qiao",
                        "suffix": ""
                    },
                    {
                        "first": "Ziwei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 17,
                "urls": [],
                "raw_text": "Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "BIBREF32",
                "title": "Breaking the quality bottleneck of video consistency model with mixed reward feedback",
                "authors": [
                    {
                        "first": "Jiachen",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Weixi",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Tsu-Jui",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Sugato",
                        "middle": [],
                        "last": "Basu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2405.18750"
                    ]
                },
                "num": 18,
                "urls": [],
                "raw_text": "Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv preprint arXiv:2405.18750, 2024.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "BIBREF40",
                "title": "Evaluation of text-to-video generation models: A dynamics perspective",
                "authors": [
                    {
                        "first": "Mingxiang",
                        "middle": [],
                        "last": "Liao",
                        "suffix": ""
                    },
                    {
                        "first": "Hannan",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Fang",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuzhong",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Wangmeng",
                        "middle": [],
                        "last": "Zuo",
                        "suffix": ""
                    },
                    {
                        "first": "Qixiang",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Jingdong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 19,
                "urls": [
                    "https://arxiv.org/abs/2407.01094"
                ],
                "raw_text": "Mingxiang Liao, Hannan Lu, Xinyu Zhang, Fang Wan, Tianyu Wang, Yuzhong Zhao, Wangmeng Zuo, Qixiang Ye, and Jingdong Wang. Evaluation of text-to-video generation models: A dynamics perspective, 2024. URL https://arxiv.org/abs/2407.01094.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "BIBREF30",
                "title": "Common diffusion noise schedules and sample steps are flawed",
                "authors": [
                    {
                        "first": "Shanchuan",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Bingchen",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jiashi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision",
                "volume": "",
                "issue": "",
                "pages": "5404--5411",
                "other_ids": {},
                "num": 20,
                "urls": [],
                "raw_text": "Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 5404\u20135411, 2024.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "BIBREF22",
                "title": "Microsoft coco: Common objects in context",
                "authors": [
                    {
                        "first": "Tsung-Yi",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Maire",
                        "suffix": ""
                    },
                    {
                        "first": "Serge",
                        "middle": [],
                        "last": "Belongie",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Hays",
                        "suffix": ""
                    },
                    {
                        "first": "Pietro",
                        "middle": [],
                        "last": "Perona",
                        "suffix": ""
                    },
                    {
                        "first": "Deva",
                        "middle": [],
                        "last": "Ramanan",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Doll\u00e1r",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lawrence",
                        "suffix": ""
                    },
                    {
                        "first": "Zitnick",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Computer Vision-ECCV 2014: 13th European Conference",
                "volume": "",
                "issue": "",
                "pages": "740--755",
                "other_ids": {},
                "num": 21,
                "urls": [],
                "raw_text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740\u2013755. Springer, 2014.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "BIBREF42",
                "title": "",
                "authors": [],
                "year": null,
                "venue": "OpenAI. Gpt-4",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 22,
                "urls": [],
                "raw_text": "OpenAI. Gpt-4o. 2024a.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "BIBREF7",
                "title": "",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Openai",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sora",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 23,
                "urls": [
                    "https://openai.com/index/sora/"
                ],
                "raw_text": "OpenAI. Sora. 2024b. URL https://openai.com/index/sora/.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "BIBREF6",
                "title": "Scalable diffusion models with transformers",
                "authors": [
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Peebles",
                        "suffix": ""
                    },
                    {
                        "first": "Saining",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "4195--4205",
                "other_ids": {},
                "num": 24,
                "urls": [],
                "raw_text": "William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4195\u20134205, 2023.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "BIBREF9",
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Matena",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqi",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of machine learning research",
                "volume": "21",
                "issue": "140",
                "pages": "1--67",
                "other_ids": {},
                "num": 25,
                "urls": [],
                "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "BIBREF11",
                "title": "High-resolution image synthesis with latent diffusion models",
                "authors": [
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Rombach",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Blattmann",
                        "suffix": ""
                    },
                    {
                        "first": "Dominik",
                        "middle": [],
                        "last": "Lorenz",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Esser",
                        "suffix": ""
                    },
                    {
                        "first": "Bj\u00f6rn",
                        "middle": [],
                        "last": "Ommer",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "10684--10695",
                "other_ids": {},
                "num": 26,
                "urls": [],
                "raw_text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684\u201310695, 2022.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "BIBREF36",
                "title": "",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 27,
                "urls": [
                    "https://runwayml.com/ai-tools/gen-2-text-to-video"
                ],
                "raw_text": "runway. Gen-2. 2023. URL https://runwayml.com/ai-tools/gen-2-text-to-video.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "BIBREF29",
                "title": "Progressive distillation for fast sampling of diffusion models",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Salimans",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2202.00512"
                    ]
                },
                "num": 28,
                "urls": [],
                "raw_text": "Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "BIBREF4",
                "title": "Make-a-video: Text-to-video generation without text-video data",
                "authors": [
                    {
                        "first": "Uriel",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Polyak",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Hayes",
                        "suffix": ""
                    },
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "An",
                        "suffix": ""
                    },
                    {
                        "first": "Songyang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Qiyuan",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Harry",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Oron",
                        "middle": [],
                        "last": "Ashual",
                        "suffix": ""
                    },
                    {
                        "first": "Oran",
                        "middle": [],
                        "last": "Gafni",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2209.14792"
                    ]
                },
                "num": 29,
                "urls": [],
                "raw_text": "Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "BIBREF14",
                "title": "Roformer: Enhanced transformer with rotary position embedding",
                "authors": [
                    {
                        "first": "Jianlin",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Murtadha",
                        "middle": [],
                        "last": "Ahmed",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Shengfeng",
                        "middle": [],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Wen",
                        "middle": [],
                        "last": "Bo",
                        "suffix": ""
                    },
                    {
                        "first": "Yunfeng",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Neurocomputing",
                "volume": "568",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 30,
                "urls": [],
                "raw_text": "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "BIBREF43",
                "title": "",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "I"
                        ],
                        "last": "Kuaishou",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Team",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kling",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 31,
                "urls": [
                    "https://kling.kuaishou.com/en"
                ],
                "raw_text": "Kuaishou AI Team. Kling. 2024. URL https://kling.kuaishou.com/en.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "BIBREF27",
                "title": "Llama 2: Open foundation and fine-tuned chat models",
                "authors": [
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Touvron",
                        "suffix": ""
                    },
                    {
                        "first": "Louis",
                        "middle": [],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Stone",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Albert",
                        "suffix": ""
                    },
                    {
                        "first": "Amjad",
                        "middle": [],
                        "last": "Almahairi",
                        "suffix": ""
                    },
                    {
                        "first": "Yasmine",
                        "middle": [],
                        "last": "Babaei",
                        "suffix": ""
                    },
                    {
                        "first": "Nikolay",
                        "middle": [],
                        "last": "Bashlykov",
                        "suffix": ""
                    },
                    {
                        "first": "Soumya",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Prajjwal",
                        "middle": [],
                        "last": "Bhargava",
                        "suffix": ""
                    },
                    {
                        "first": "Shruti",
                        "middle": [],
                        "last": "Bhosale",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2307.09288"
                    ]
                },
                "num": 32,
                "urls": [],
                "raw_text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
                "links": null
            },
            "BIBREF0": {
                "ref_id": "BIBREF0",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "30",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 33,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "BIBREF3",
                "title": "Phenaki: Variable length video generation from open domain textual descriptions",
                "authors": [
                    {
                        "first": "Ruben",
                        "middle": [],
                        "last": "Villegas",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Babaeizadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Pieter-Jan",
                        "middle": [],
                        "last": "Kindermans",
                        "suffix": ""
                    },
                    {
                        "first": "Hernan",
                        "middle": [],
                        "last": "Moraldo",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Taghi Saffar",
                        "suffix": ""
                    },
                    {
                        "first": "Santiago",
                        "middle": [],
                        "last": "Castro",
                        "suffix": ""
                    },
                    {
                        "first": "Julius",
                        "middle": [],
                        "last": "Kunze",
                        "suffix": ""
                    },
                    {
                        "first": "Dumitru",
                        "middle": [],
                        "last": "Erhan",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 34,
                "urls": [],
                "raw_text": "Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "BIBREF24",
                "title": "Cogvlm: Visual expert for pretrained language models",
                "authors": [
                    {
                        "first": "Weihan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Qingsong",
                        "middle": [],
                        "last": "Lv",
                        "suffix": ""
                    },
                    {
                        "first": "Wenmeng",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenyi",
                        "middle": [],
                        "last": "Hong",
                        "suffix": ""
                    },
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Junhui",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Xixuan",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2311.03079"
                    ]
                },
                "num": 35,
                "urls": [],
                "raw_text": "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023a.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "BIBREF38",
                "title": "Lavie: High-quality video generation with cascaded latent diffusion models",
                "authors": [
                    {
                        "first": "Yaohui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyuan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Shangchen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Ziqi",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ceyuan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Yinan",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jiashuo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Peiqing",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2309.15103"
                    ]
                },
                "num": 36,
                "urls": [],
                "raw_text": "Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023b.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "BIBREF18",
                "title": "Effective long-context scaling of foundation models",
                "authors": [
                    {
                        "first": "Wenhan",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Jingyu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Igor",
                        "middle": [],
                        "last": "Molybog",
                        "suffix": ""
                    },
                    {
                        "first": "Hejia",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Prajjwal",
                        "middle": [],
                        "last": "Bhargava",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Louis",
                        "middle": [],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": "Rashi",
                        "middle": [],
                        "last": "Rungta",
                        "suffix": ""
                    },
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Abinav Sankararaman",
                        "suffix": ""
                    },
                    {
                        "first": "Barlas",
                        "middle": [],
                        "last": "Oguz",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2309.16039"
                    ]
                },
                "num": 37,
                "urls": [],
                "raw_text": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "BIBREF10",
                "title": "Language model beats diffusion-tokenizer is key to visual generation",
                "authors": [
                    {
                        "first": "Lijun",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Jos\u00e9",
                        "middle": [],
                        "last": "Lezama",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Nitesh",
                        "suffix": ""
                    },
                    {
                        "first": "Luca",
                        "middle": [],
                        "last": "Gundavarapu",
                        "suffix": ""
                    },
                    {
                        "first": "Kihyuk",
                        "middle": [],
                        "last": "Versari",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Sohn",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Minnen",
                        "suffix": ""
                    },
                    {
                        "first": "Agrim",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiuye",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "G"
                        ],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Hauptmann",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2310.05737"
                    ]
                },
                "num": 38,
                "urls": [],
                "raw_text": "Lijun Yu, Jos\u00e9 Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion\u2013tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "BIBREF41",
                "title": "Chronomagic-bench: A benchmark for metamorphic evaluation of text-to-time-lapse video generation",
                "authors": [
                    {
                        "first": "Shenghai",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Jinfa",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Yongqi",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Yaoyang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Shaofeng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yujun",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Ruijie",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Xinhua",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Jiebo",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2406.18522"
                    ]
                },
                "num": 39,
                "urls": [],
                "raw_text": "Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: A benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv preprint arXiv:2406.18522, 2024.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "BIBREF35",
                "title": "Show-1: Marrying pixel and latent diffusion models for text-to-video generation",
                "authors": [
                    {
                        "first": "David Junhao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [
                            "Zhangjie"
                        ],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jia-Wei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Lingmin",
                        "middle": [],
                        "last": "Ran",
                        "suffix": ""
                    },
                    {
                        "first": "Yuchao",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Difei",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [
                            "Zheng"
                        ],
                        "last": "Shou",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2309.15818"
                    ]
                },
                "num": 40,
                "urls": [],
                "raw_text": "David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023a.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "BIBREF20",
                "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
                "authors": [
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Lidong",
                        "middle": [],
                        "last": "Bing",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2306.02858"
                    ]
                },
                "num": 41,
                "urls": [],
                "raw_text": "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023b.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "BIBREF13",
                "title": "The unreasonable effectiveness of deep features as a perceptual metric",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Phillip",
                        "middle": [],
                        "last": "Isola",
                        "suffix": ""
                    },
                    {
                        "first": "Alexei",
                        "middle": [
                            "A"
                        ],
                        "last": "Efros",
                        "suffix": ""
                    },
                    {
                        "first": "Eli",
                        "middle": [],
                        "last": "Shechtman",
                        "suffix": ""
                    },
                    {
                        "first": "Oliver",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "586--595",
                "other_ids": {},
                "num": 42,
                "urls": [],
                "raw_text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586\u2013595, 2018.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "BIBREF26",
                "title": "Cogview3: Finer and faster text-to-image generation via relay diffusion",
                "authors": [
                    {
                        "first": "Wendi",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Jiayan",
                        "middle": [],
                        "last": "Teng",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Weihan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jidong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaotao",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxiao",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2403.05121"
                    ]
                },
                "num": 43,
                "urls": [],
                "raw_text": "Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogview3: Finer and faster text-to-image generation via relay diffusion. arXiv preprint arXiv:2403.05121, 2024a.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "BIBREF34",
                "title": "Open-sora: Democratizing efficient video production for all",
                "authors": [
                    {
                        "first": "Zangwei",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Tianji",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Chenhui",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Shenggui",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Hongxin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yukun",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "You",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": 44,
                "urls": [
                    "https://github.com/hpcaitech/Open-Sora"
                ],
                "raw_text": "Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024b. URL https://github.com/hpcaitech/Open-Sora.",
                "links": null
            }
        },
        "ref_entries": {
            "EQREF34": {
                "text": "L  simple  (\u03b8):=\ud835\udc04 t,x 0 ,\u03f5 \u03f5-\u03f5 \u03b8 (\u03b1 \u00af t x 0 +1-\u03b1 \u00af t \u03f5,t) 2 ,",
                "num": "1",
                "latex": "~\nL_\\mathrm {simple}(\\theta ) := \\mathbf {E}_{t, x_0, \\epsilon }{ \\left\\Vert  \\epsilon - \\epsilon _\\theta (\\sqrt{\\bar{\\alpha }_t} x_0 + \\sqrt{1-\\bar{\\alpha }_t}\\epsilon , t) \\right\\Vert ^2},",
                "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><mrow><mi>~</mi><msub><mi>L</mi><mi>\\mathrm</mi></msub><mrow><mi>s</mi><mi>i</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi></mrow><mo stretchy=\"false\">&#x00028;</mo><mi>&#x003B8;</mi><mo stretchy=\"false\">&#x00029;</mo><mi>:</mi><mo>&#x0003D;</mo><msub><mrow><mi>E</mi></mrow><mrow><mi>t</mi><mi>,</mi><msub><mi>x</mi><mn>0</mn></msub><mi>,</mi><mi>&#x003F5;</mi></mrow></msub><mrow><msup><mrow><mo stretchy=\"true\" fence=\"true\" form=\"prefix\">&#x02016;</mo><mrow><mi>&#x003F5;</mi><mo>&#x02212;</mo><msub><mi>&#x003F5;</mi><mi>&#x003B8;</mi></msub><mo stretchy=\"false\">&#x00028;</mo><msqrt><mrow><mover><mi>_</mi><mo stretchy=\"true\">&#x000AF;</mo></mover><mrow><mi>&#x003B1;</mi></mrow><mi>t</mi></mrow></msqrt><msub><mi>x</mi><mn>0</mn></msub><mo>&#x0002B;</mo><msqrt><mrow><mn>1</mn><mo>&#x02212;</mo><mover><mi>_</mi><mo stretchy=\"true\">&#x000AF;</mo></mover><mrow><mi>&#x003B1;</mi></mrow><mi>t</mi></mrow></msqrt><mi>&#x003F5;</mi><mi>,</mi><mi>t</mi><mo stretchy=\"false\">&#x00029;</mo></mrow><mo stretchy=\"true\" fence=\"true\" form=\"postfix\">&#x02016;</mo></mrow><mn>2</mn></msup></mrow><mi>,</mi></mrow></math>",
                "type_str": "equation"
            },
            "FIGREF1": {
                "text": "The performance of openly-accessible text-to-video models in different aspects.",
                "uris": [
                    "images/bench_eval9.png"
                ],
                "num": "1",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "text": "The overall architecture of CogVideoX.",
                "uris": [
                    "images/transformer.png"
                ],
                "num": "2",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "text": "(a) The structure of the 3D VAE in CogVideoX. It comprises an encoder, a decoder and a latent space regularizer, achieving a 4\u00d78\u00d78 compression from pixels to the latents. (b) The context parallel implementation on the temporally causal convolution.",
                "uris": [
                    "images/3dvae_combined.jpg"
                ],
                "num": "3",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF8": {
                "text": "RoPE vs. Sinusoidal 0.460.480.460.48(a) b(b) b(c) b(d) bRoPE vs. RoPE + LearnableExpert Ada. LN vs. Expert Ada. LN + MLPUniform Sampling vs. No Uniform SamplingTraining loss curve of different ablations.",
                "uris": [],
                "num": "4",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF14": {
                "text": "The separated spatial and temporal attention makes it challenging to handle the large motion between adjacent frames. In the figure, the head of the person in frame i+1 cannot directly attend to the head in frame i. Instead, visual information can only be implicitly transmitted through other background patches. This can lead to inconsistency issues in the generated videos.",
                "uris": [
                    "images/attention.jpg"
                ],
                "num": "5",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF25": {
                "text": "The pipeline for dense video caption data generation. In this pipeline, we generate short video captions with the Panda70M model, extract frames to create dense image captions, and use GPT-4 to summarize these into final video captions. To accelerate this process, we fine-tuned a Llama 2 model with the GPT-4 summaries.",
                "uris": [
                    "images/pipeline.jpg"
                ],
                "num": "6",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF27": {
                "text": "The diagram of mixed-duration training and Frame Pack. To fully utilize the data and enhance the model's generalization capability, we train with videos of different durations within the same batch.",
                "uris": [
                    "images/CogVideoX.png"
                ],
                "num": "7",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF30": {
                "text": "The comparison between the initial generation states of extrapolation and interpolation when increasing the resolution with RoPE encoding. Extrapolation tends to generate multiple small, clear, and repetitive images, while interpolation generates a blurry large image.",
                "uris": [
                    "images/ive.jpg"
                ],
                "num": "8",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF41": {
                "text": "Text to video showcases. The displayed prompt will be upsampled before being fed into the model. The generated videos contain large motion and can produce various video styles.",
                "uris": [
                    "images/t2v/goodcase1.jpg"
                ],
                "num": "9",
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF42": {
                "text": "Text to video showcases.",
                "uris": [
                    "images/t2v/goodcase2.jpg"
                ],
                "num": "10",
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF43": {
                "text": "Evaluation results.",
                "num": "1",
                "html": "<table><tr><th>Models</th><th>Human</th></tr><tr><td>Action</td><td>Scene</td><td>Dynamic</td></tr><tr><td>Degree</td><td>Multiple</td></tr><tr><td>Objects</td><td>Appearance</td></tr><tr><td>Style</td><td>Dynamic</td></tr><tr><td>Quality</td><td>GPT4o-MT</td></tr><tr><td>Score</td></tr><tr><td>T2V-Turbo</td><td>95.2</td><td>55.58</td><td>49.17</td><td>54.65</td><td>24.42</td><td>\u2013</td><td>\u2013</td></tr><tr><td>AnimateDiff</td><td>92.6</td><td>50.19</td><td>40.83</td><td>36.88</td><td>22.42</td><td>\u2013</td><td>2.62</td></tr><tr><td>VideoCrafter-2.0</td><td>95.0</td><td>55.29</td><td>42.50</td><td>40.66</td><td>25.13</td><td>43.6</td><td>2.68</td></tr><tr><td>OpenSora V1.2</td><td>85.8</td><td>42.47</td><td>47.22</td><td>58.41</td><td>23.89</td><td>63.7</td><td>2.52</td></tr><tr><td>Show-1</td><td>95.6</td><td>47.03</td><td>44.44</td><td>45.47</td><td>23.06</td><td>57.7</td><td>\u2013</td></tr><tr><td>Gen-2</td><td>89.2</td><td>48.91</td><td>18.89</td><td>55.47</td><td>19.34</td><td>43.6</td><td>2.62</td></tr><tr><td>Pika</td><td>88.0</td><td>44.80</td><td>37.22</td><td>46.69</td><td>21.89</td><td>52.1</td><td>2.48</td></tr><tr><th>LaVie-2</th><th>96.4</th><th>49.59</th><th>31.11</th><th>64.88</th><th>25.09</th><th>\u2013</th><th>2.46</th></tr><tr><td>CogVideoX-2B</td><td>88.0</td><td>39.94</td><td>63.33</td><td>53.70</td><td>23.67</td><td>57.7</td><td>3.09</td></tr><tr><td>CogVideoX-5B</td><td>96.8</td><td>55.44</td><td>62.22</td><td>70.95</td><td>24.44</td><td>69.5</td><td>3.36</td></tr></table>",
                "content": [
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Models",
                                "latex": "Models"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Human",
                                "latex": "Human"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Action",
                                "latex": "Action"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Scene",
                                "latex": "Scene"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Dynamic",
                                "latex": "Dynamic"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Degree",
                                "latex": "Degree"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Multiple",
                                "latex": "Multiple"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Objects",
                                "latex": "Objects"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Appearance",
                                "latex": "Appearance"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Style",
                                "latex": "Style"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Dynamic",
                                "latex": "Dynamic"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Quality",
                                "latex": "Quality"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "GPT4o-MT",
                                "latex": "GPT4o-MT"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Score",
                                "latex": "Score"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "T2V-Turbo",
                                "latex": "T2V-Turbo"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "95.2",
                                "latex": "95.2"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "55.58",
                                "latex": "55.58"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "49.17",
                                "latex": "49.17"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "54.65",
                                "latex": "54.65"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "24.42",
                                "latex": "24.42"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "\u2013",
                                "latex": "\u2013"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "\u2013",
                                "latex": "\u2013"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "AnimateDiff",
                                "latex": "AnimateDiff"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "92.6",
                                "latex": "92.6"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "50.19",
                                "latex": "50.19"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "40.83",
                                "latex": "40.83"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "36.88",
                                "latex": "36.88"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "22.42",
                                "latex": "22.42"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "\u2013",
                                "latex": "\u2013"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "2.62",
                                "latex": "2.62"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "VideoCrafter-2.0",
                                "latex": "VideoCrafter-2.0"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "95.0",
                                "latex": "95.0"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "55.29",
                                "latex": "55.29"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "42.50",
                                "latex": "42.50"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "40.66",
                                "latex": "40.66"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "25.13",
                                "latex": "25.13"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "43.6",
                                "latex": "43.6"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "2.68",
                                "latex": "2.68"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "OpenSora V1.2",
                                "latex": "OpenSora V1.2"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "85.8",
                                "latex": "85.8"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "42.47",
                                "latex": "42.47"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "47.22",
                                "latex": "47.22"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "58.41",
                                "latex": "58.41"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "23.89",
                                "latex": "23.89"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "63.7",
                                "latex": "63.7"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "2.52",
                                "latex": "2.52"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Show-1",
                                "latex": "Show-1"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "95.6",
                                "latex": "95.6"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "47.03",
                                "latex": "47.03"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "44.44",
                                "latex": "44.44"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "45.47",
                                "latex": "45.47"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "23.06",
                                "latex": "23.06"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "57.7",
                                "latex": "57.7"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "\u2013",
                                "latex": "\u2013"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Gen-2",
                                "latex": "Gen-2"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "89.2",
                                "latex": "89.2"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "48.91",
                                "latex": "48.91"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "18.89",
                                "latex": "18.89"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "55.47",
                                "latex": "55.47"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "19.34",
                                "latex": "19.34"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "43.6",
                                "latex": "43.6"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "2.62",
                                "latex": "2.62"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Pika",
                                "latex": "Pika"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "88.0",
                                "latex": "88.0"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "44.80",
                                "latex": "44.80"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "37.22",
                                "latex": "37.22"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "46.69",
                                "latex": "46.69"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "21.89",
                                "latex": "21.89"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "52.1",
                                "latex": "52.1"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "2.48",
                                "latex": "2.48"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": true,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "LaVie-2",
                                "latex": "LaVie-2"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "96.4",
                                "latex": "96.4"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "49.59",
                                "latex": "49.59"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "31.11",
                                "latex": "31.11"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "64.88",
                                "latex": "64.88"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "25.09",
                                "latex": "25.09"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "\u2013",
                                "latex": "\u2013"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "2.46",
                                "latex": "2.46"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "CogVideoX-2B",
                                "latex": "CogVideoX-2B"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "88.0",
                                "latex": "88.0"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "39.94",
                                "latex": "39.94"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "63.33",
                                "latex": "63.33"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "53.70",
                                "latex": "53.70"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "23.67",
                                "latex": "23.67"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "57.7",
                                "latex": "57.7"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "3.09",
                                "latex": "3.09"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "CogVideoX-5B",
                                "latex": "CogVideoX-5B"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "96.8",
                                "latex": "96.8"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "55.44",
                                "latex": "55.44"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "62.22",
                                "latex": "62.22"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "70.95",
                                "latex": "70.95"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "24.44",
                                "latex": "24.44"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "69.5",
                                "latex": "69.5"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "3.36",
                                "latex": "3.36"
                            }
                        ]
                    }
                ],
                "type_str": "table"
            },
            "TABREF45": {
                "text": "Human evaluation between CogVideoX and Kling.",
                "num": "2",
                "html": "<table><tr><th>Model</th><th>Sensory</th></tr><tr><td>Quality</td><td>Instruction</td></tr><tr><td>Following</td><td>Physics</td></tr><tr><td>Simulation</td><td>Cover</td></tr><tr><td>Quality</td><td>Total</td></tr><tr><td>Score</td></tr><tr><td>Kling</td><td>0.638</td><td>0.367</td><td>0.561</td><td>0.668</td><td>2.17</td></tr><tr><td>CogVideoX-5B</td><td>0.722</td><td>0.495</td><td>0.667</td><td>0.712</td><td>2.74</td></tr></table>",
                "content": [
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Model",
                                "latex": "Model"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Sensory",
                                "latex": "Sensory"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Quality",
                                "latex": "Quality"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Instruction",
                                "latex": "Instruction"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Following",
                                "latex": "Following"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Physics",
                                "latex": "Physics"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Simulation",
                                "latex": "Simulation"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Cover",
                                "latex": "Cover"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Quality",
                                "latex": "Quality"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Total",
                                "latex": "Total"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Score",
                                "latex": "Score"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "Kling",
                                "latex": "Kling"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "0.638",
                                "latex": "0.638"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "0.367",
                                "latex": "0.367"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "0.561",
                                "latex": "0.561"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "0.668",
                                "latex": "0.668"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "2.17",
                                "latex": "2.17"
                            }
                        ]
                    },
                    {
                        "top-border": false,
                        "bottom-border": false,
                        "cells": [
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "CogVideoX-5B",
                                "latex": "CogVideoX-5B"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "0.722",
                                "latex": "0.722"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "0.495",
                                "latex": "0.495"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "0.667",
                                "latex": "0.667"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "0.712",
                                "latex": "0.712"
                            },
                            {
                                "alignment": "center",
                                "right-border": false,
                                "left-border": false,
                                "text": "2.74",
                                "latex": "2.74"
                            }
                        ]
                    }
                ],
                "type_str": "table"
            },
            "FOOTREF26": {
                "text": "The CogVLM2-Video model weight is openly available at https://github.com/THUDM/CogVLM2 .",
                "type_str": "footnote",
                "num": "1"
            },
            "SECREF1": {
                "text": "Introduction",
                "parent": null,
                "type_str": "section",
                "num": "1"
            },
            "SECREF2": {
                "text": "The CogVideoX Architecture",
                "parent": null,
                "type_str": "section",
                "num": "2"
            },
            "SECREFU3": {
                "text": "3D Causal VAE",
                "parent": "SECREF2",
                "type_str": "section",
                "num": "2.1"
            },
            "SECREFU5": {
                "text": "Expert Transformer",
                "parent": "SECREF2",
                "type_str": "section",
                "num": "2.2"
            },
            "SECREFU6": {
                "text": "Patchify.",
                "parent": "SECREFU5",
                "type_str": "section",
                "num": "2.2.0.1"
            },
            "SECREFU7": {
                "text": "3D-RoPE.",
                "parent": "SECREFU5",
                "type_str": "section",
                "num": "2.2.0.2"
            },
            "SECREFU13": {
                "text": "Expert Transformer Block.",
                "parent": "SECREFU5",
                "type_str": "section",
                "num": "2.2.0.3"
            },
            "SECREFU15": {
                "text": "3D Full Attention.",
                "parent": "SECREFU5",
                "type_str": "section",
                "num": "2.2.0.4"
            },
            "SECREFU16": {
                "text": "Data",
                "parent": "SECREF2",
                "type_str": "section",
                "num": "2.3"
            },
            "SECREFU17": {
                "text": "Video Filtering.",
                "parent": "SECREFU16",
                "type_str": "section",
                "num": "2.3.0.5"
            },
            "SECREFU24": {
                "text": "Video Caption.",
                "parent": "SECREFU16",
                "type_str": "section",
                "num": "2.3.0.6"
            },
            "SECREF3": {
                "text": "Training CogVideoX",
                "parent": null,
                "type_str": "section",
                "num": "3"
            },
            "SECREFU28": {
                "text": "Frame Pack",
                "parent": "SECREF3",
                "type_str": "section",
                "num": "3.1"
            },
            "SECREFU29": {
                "text": "Resolution Progressive Training",
                "parent": "SECREF3",
                "type_str": "section",
                "num": "3.2"
            },
            "SECREFU31": {
                "text": "Extrapolation of Position Code.",
                "parent": "SECREFU29",
                "type_str": "section",
                "num": "3.2.0.7"
            },
            "SECREFU32": {
                "text": "High-Quality Fine-Tuning.",
                "parent": "SECREFU29",
                "type_str": "section",
                "num": "3.2.0.8"
            },
            "SECREFU33": {
                "text": "Explicit Uniform Sampling",
                "parent": "SECREF3",
                "type_str": "section",
                "num": "3.3"
            },
            "SECREFU35": {
                "text": "Position Embedding",
                "parent": "SECREF3",
                "type_str": "section",
                "num": "3.4"
            },
            "SECREFU36": {
                "text": "Expert Adaptive Layernorm",
                "parent": "SECREF3",
                "type_str": "section",
                "num": "3.5"
            },
            "SECREF4": {
                "text": "Empirical Evaluation",
                "parent": null,
                "type_str": "section",
                "num": "4"
            },
            "SECREFU37": {
                "text": "Automated Metric Evaluation",
                "parent": "SECREF4",
                "type_str": "section",
                "num": "4.1"
            },
            "SECREFU38": {
                "text": "Baselines.",
                "parent": "SECREFU37",
                "type_str": "section",
                "num": "4.1.0.9"
            },
            "SECREFU39": {
                "text": "Evaluation Metrics.",
                "parent": "SECREFU37",
                "type_str": "section",
                "num": "4.1.0.10"
            },
            "SECREFU40": {
                "text": "Results.",
                "parent": "SECREFU37",
                "type_str": "section",
                "num": "4.1.0.11"
            },
            "SECREFU44": {
                "text": "Human Evaluation",
                "parent": "SECREF4",
                "type_str": "section",
                "num": "4.2"
            },
            "SECREF1006": {
                "text": "Acknowledgments",
                "parent": "SECREFU44",
                "type_str": "section",
                "num": null
            }
        }
    }
}