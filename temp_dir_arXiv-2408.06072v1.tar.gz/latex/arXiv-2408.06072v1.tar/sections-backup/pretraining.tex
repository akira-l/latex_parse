\section{Training}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\linewidth]{images/CogVideoX.png}
\end{center}
\caption{
The diagram of Mixed duration training and Frame Pack. To fully utilize the data and enhance the model's generalization capability, we train with videos of different durations within the same batch.}
\label{fig:framepack}
\end{figure}

\subsection{Setting}
We mixed images and videos during training, treating each image as a single-frame video. Additionally, we employed progressive training from the resolution perspective. For the diffusion setting, we adopt v-prediction~\citep{salimans2022progressive} and zero SNR~\citep{lin2024common}, following the noise schedule used in LDM~\citep{rombach2022high}.
During diffusion training for timestep sampling, we also employed an explicit uniform timestep sampling method, which benefits stable training. 

\subsection{Frame Pack}
Previous video model training methods often involved jointly training images and fixed frames videos. However, this approach led to two major problems: 1. There is a significant gap between the two input types using bidirectional attention, with images having 1 frame while videos having dozens of frames. We observed that models trained this way tend to diverge into two generative modes based on the token number and do not generalize well. 2. For training with a fixed duration, we need to discard short videos and truncate long videos, which prevents full utilization of the videos of varying frames.

To address these issues, we chose mixed-duration training, which means training videos of different lengths together. However, inconsistent data shapes within the batch make training difficult. Inspired by Patch'n Pack \citep{dehghani2024patch}, we place videos of different lengths into the same batch to ensure consistent shapes within each batch, a method we refer to as Frame Pack. 

\subsection{resolution prograssive training}
Our training pipeline is divided into three stages: low-resolution training, high-resolution training, and high-quality video fine-tuning. Similar to images, internet videos also include a significant amount of low-resolution videos. Prograssive training can effectively utilize various resolution videos. 
Moreover, training at low resolution initially can equip the model with coarse-grained modeling capabilities, followed by high-resolution training to enhance the model's ability to capture fine details. Compared to direct high-resolution training, staged training can reduce the overall training time.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\linewidth]{images/ive.png}
\end{center}
\caption{We compared the initial generation states of extrapolation and interpolation when increasing resolution with RoPE encoding. Extrapolation tends to generate multiple small, clear, and repetitive images, while interpolation generates a blurry large image.}
\label{fig:ive}
\end{figure}

\paragraph{Extrapolation of position code}
When adapting low-resolution position encoding to high-resolution, we consider two different methods: interpolation and extrapolation. We show the effects of two methods in Figure~\ref{fig:ive}. Interpolation tens to preserve global information more effectively, whereas the extrapolation better retains local details. Given that RoPE is a relative position encoding, We chose the extrapolation to maintain the relative position between pixels. 

\paragraph{High-Quality fine-tuning}
Since the filtered pre-training data still contains a certain proportion of dirty data, such as subtitles, watermarks, and low-bitrate videos, we selected a subset of higher quality video data, accounting for 20\% of the total dataset, for fine-tuning in the final stage. This step effectively removed generated subtitles and watermarks and slightly improved the visual quality. However, we also observed a slight degradation in the model's semantic ability.


\subsection{Explicit Uniform Sampling}
~\citet{ho2020denoising} defines the training objective of diffusion as 
\begin{equation}~\label{eq:ddpm-loss}
    L_\mathrm{simple}(\theta) := \mathbf{E}_{t, x_0, \epsilon}{ \left\| \epsilon - \epsilon_\theta(\sqrt{\bar\alpha_t} x_0 + \sqrt{1-\bar\alpha_t}\epsilon, t) \right\|^2},
\end{equation}
where $t$ is uniformly distributed between 1 and T.
The common practice is for each rank in the data parallel group to uniformly sample a value between 1 and 
T. In theory, this is equivalent to Equation~\ref{eq:ddpm-loss}. However, in practice, the results obtained from such random sampling are often not sufficiently uniform, and since the magnitude of the diffusion loss is related to the timesteps, this can lead to significant fluctuations in the loss.
$Explicit\ Uniform\ Sampling$ is to divide the range from 1 to T into n intervals, where n is the number of ranks. Each rank then uniformly samples within its respective interval. This method ensures a more uniform distribution of timesteps. As shown in Figure~\ref{fig:a}, the loss curve from training with Explicit Uniform Sampling is noticeably more stable.
