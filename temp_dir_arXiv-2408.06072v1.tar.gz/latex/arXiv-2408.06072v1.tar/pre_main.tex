
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx} 
\usepackage[most]{tcolorbox}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{array}
\usepackage[usestackEOL]{stackengine}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{xspace}

\newcommand{\hide}[1]{}
\newcommand{\model}{CogVideoX\xspace}
\newcommand{\dong}[1]{\textbf{\color{red}[(Dong: #1 )]}}

\newtcolorbox{promptbox}[1][]{
  breakable,
  title=#1,
  colback=gray!5,
  colframe=black,
  colbacktitle=gray!15,
  coltitle=black,
  fonttitle=\bfseries,
  bottomrule=1.5pt,
  toprule=1.5pt,
  leftrule=1pt,
  rightrule=1pt,
  arc=0pt,
  outer arc=0pt,
  enhanced,
  before upper={\parindent=1.5em} % This sets the paragraph indentation
}

\lstset{
  basicstyle=\ttfamily,
  breaklines=true,
  breakatwhitespace=false,
  columns=flexible,
  keepspaces=true,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=4,
  xleftmargin=0pt,
  xrightmargin=0pt
}

\newcommand{\aspace}{\hspace{1em}}


\title{
\includegraphics[width=0.3\textwidth]{images/logo.png}
%CogVideoX: 
A Text-to-Video Diffusion Model with Expert Transformer}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{Zhuoyi Yang*\aspace Jiayan Teng* \aspace Wendi Zheng\aspace Ming Ding\aspace Shiyu Huang \aspace Jiazheng Xu \aspace \\
\textbf{Yuanming Yang \aspace Xiaohan Zhang \aspace Xiaotao Gu 
\aspace Guanyu Feng \aspace  Da Yin \aspace Wenyi Hong \aspace } \\
\textbf{ Weihan Wang \aspace Yean Cheng \aspace Yuxuan Zhang \aspace Ting Liu \aspace Bin Xu \aspace Yuxiao Dong \aspace Jie Tang} \\
$^1$Zhipu AI\ \ \ \ \ \ $^2$Tsinghua University \\
\href{https://github.com/THUDM/CogVideo}{https://github.com/THUDM/CogVideo}
}

 
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\renewcommand{\thefootnote}{}
\footnotetext{*equal contributions. Core contributors: Zhuoyi, Jiayan, Wendi, Ming, and Shiyu.}
\footnotetext{{\{yangzy22,tengjy20\}@mails.tsinghua.edu.cn, \{yuxiaod,jietang\}@tsinghua.edu.cn}}
%\footnotetext{$^\dagger$ corresponding authors}
\renewcommand{\thefootnote}{\arabic{footnote}}
\begin{abstract}
We introduce CogVideoX, a large diffusion transformer model capable of generating videos conditioned on text prompts. It applies a 3D VAE and a 3D transformer based on Expert Adaptive LayerNorm. Employing a progressive training technique, the model is able to generate coherent long-duration videos characterized by significant motion. Additionally, we propose a complete large-scale data processing pipeline, including various data cleaning strategies and video re-caption method, resulting in better generation quality and improved semantic alignment. Finally, CogVideoX achieves state-of-the-art performance across multiple machine metrics and human evaluations.
\end{abstract}

\input{sections/introduction}
\input{sections/vae}
\input{sections/model}
\input{sections/data}
\input{sections/pretraining}
\input{sections/abaltion}
\input{sections/results}

 
\subsubsection*{Acknowledgments}
%This research was supported by Zhipu AI. Thanks to BiliBili for data support. Thanks to all our collaborators and partners from Knowledge Engineering Group (KEG) and Zhipu AI.

We would like to thank all the data annotators, infra operating staffs, collaborators, and partners as well as everyone at Zhipu AI and Tsinghua University not explicitly mentioned in the report who have provided support, feedback, and contributed to \model. 
We would also like to greatly thank BiliBili for data support. 
%We would also like to thank Yuxuan Zhang and Wei Jia from Zhipu AI as well as the teams at Hugging Face, ModelScope, WiseModel, and others for their help on the open-sourcing efforts of the GLM family of models.



\bibliography{reference}
\bibliographystyle{iclr2024_conference}

\hide{ % tmp hide of 

\clearpage
\appendix
\input{appendix/image2video}
\input{appendix/caption_upsampler}
\input{appendix/video_caption_gen}
\input{appendix/video_caption_example}
\input{appendix/video2video}
\input{appendix/human_evaluation}
}% end of hide

\end{document}
