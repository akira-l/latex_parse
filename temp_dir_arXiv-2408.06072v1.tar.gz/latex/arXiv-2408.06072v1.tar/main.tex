\documentclass{article}


% ready for submission
% \usepackage{neurips_2024}
\usepackage[preprint]{neurips_2024}

\usepackage{graphicx} 
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{array}
\usepackage[usestackEOL]{stackengine}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage[most]{tcolorbox}
 
\newcommand{\hide}[1]{}

\newcommand{\model}{CogVideoX\xspace} 
\newcommand{\dong}[1]{\textbf{\color{red}[(Dong: #1 )]}}
\newcommand{\gxt}[1]{\textbf{\color{cyan}[Gu: #1 ]}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\aspace}{\hspace{1em}}

\newtcolorbox{promptbox}[1][]{
  breakable,
  title=#1,
  colback=gray!5,
  colframe=black,
  colbacktitle=gray!15,
  coltitle=black,
  fonttitle=\bfseries,
  bottomrule=1.5pt,
  toprule=1.5pt,
  leftrule=1pt,
  rightrule=1pt,
  arc=0pt,
  outer arc=0pt,
  enhanced,
  before upper={\parindent=1.5em}
}


% author formatting
\usepackage{authblk}
\renewcommand\Authands{, } %
\renewcommand{\Authfont}{\bfseries}
\makeatletter
\renewcommand\AB@affilsepx{, \protect\Affilfont}
\makeatother

\title{
\includegraphics[width=0.07\textwidth]{images/logo.png}
CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer}

\author{Zhuoyi Yang$^{\star}$ \aspace  Jiayan Teng$^{\star}$ \aspace  Wendi Zheng  \aspace Ming Ding \aspace  Shiyu Huang \aspace Jiazheng Xu \\
Yuanming Yang\aspace  Wenyi Hong\aspace  Xiaohan Zhang \aspace Guanyu Feng \aspace Da Yin  \\
\aspace Xiaotao Gu  \aspace  Yuxuan Zhang \aspace Weihan Wang  \aspace Yean Cheng  \aspace Ting Liu \aspace   Bin Xu   \aspace \\
  Yuxiao Dong \aspace  Jie Tang \\ 
~\\ 
\textnormal{Zhipu AI \aspace Tsinghua University}
}
% \affil[]{Zhipu AI \aspace Tsinghua University}
\affil[]{}


%  \hide{
% \author{Zhuoyi Yang$^{\star}$ \aspace {\bf Jiayan Teng}$^{\star}$ \aspace {\bf Wendi Zheng} \aspace {\bf Ming Ding} \aspace {\bf Shiyu Huang} \\
% \aspace {\bf Jiazheng Xu} \aspace
% {\bf Yuanming Yang} \aspace {\bf Xiaohan Zhang} \aspace {\bf Xiaotao Gu} \aspace  {\bf Guanyu Feng}\aspace \\
%  {\bf Da Yin} 
% \aspace {\bf Wenyi Hong} \aspace  {\bf Weihan Wang} \aspace
%  {\bf Yean Cheng} \aspace {\bf Yuxuan Zhang} \aspace  \\
%  {\bf Ting Liu} \aspace {\bf Bin Xu}  \aspace {\bf Yuxiao Dong} \aspace {\bf Jie Tang}~\\
% $^{1}$Zhipu AI \aspace $^{2}$Tsinghua University\\
% \textmd{\href{https://github.com/THUDM/CogVideo}{https://github.com/THUDM/CogVideo}} 
% }
% }

% $^1$Zhipu AI\ \ \ \ \ \ $^2$Tsinghua University 
% \href{https://github.com/THUDM/CogVideo}{https://github.com/THUDM/CogVideo}
 



% \renewcommand{\thefootnote}{}
% \footnotetext{Zhuoyi and Jiayan made equal contributions. Core contributors: Zhuoyi, Jiayan, Wendi, Ming, and Shiyu.}
% \footnotetext{{\{yangzy22,tengjy20\}@mails.tsinghua.edu.cn, \{yuxiaod,jietang\}@tsinghua.edu.cn}}
% %\footnotetext{$^\dagger$ corresponding authors}
% \renewcommand{\thefootnote}{\arabic{footnote}}

\begin{document}


\maketitle

\renewcommand{\thefootnote}{}
\footnotetext{*Equal contributions. Core contributors: Zhuoyi, Jiayan, Wendi, Ming, and Shiyu.}
\footnotetext{{\{yangzy22,tengjy24\}@mails.tsinghua.edu.cn, \{yuxiaod,jietang\}@tsinghua.edu.cn}}
\renewcommand{\thefootnote}{\arabic{footnote}}


\begin{abstract}
We introduce \model, large-scale diffusion transformer models designed for generating videos based on text prompts. 
To efficently model video data, we propose to levearge a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions. 
To improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. 
By employing a progressive training technique, \model is adept at producing coherent, long-duration videos characterized by significant motions. 
In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method. 
It significantly helps enhance the performance of \model, improving both generation quality and semantic alignment. 
Results show that \model demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. 
The model weights of both the 3D Causal VAE and \model are publicly available at \url{https://github.com/THUDM/CogVideo}. 
% and \url{https://huggingfaces.co/THUDM/CogVideoX}. 

%We introduce CogVideoX, a large-scale diffusion transformer model designed for generating videos based on text prompts. The model leverages a 3D Variational Autoencoder (VAE) and a 3D expert transformer, utilizing Expert Adaptive LayerNorm. By employing a progressive training technique, CogVideoX is adept at producing coherent, long-duration videos characterized by significant motion. Additionally, we introduce a comprehensive large-scale data processing pipeline that includes various data cleaning strategies and a novel video captioning method, resulting in enhanced generation quality and improved semantic alignment. CogVideoX demonstrates state-of-the-art performance across multiple machine metrics and human evaluations.
\end{abstract}

\input{sections/introduction}
\input{sections/vae}
\input{sections/model}
\input{sections/pretraining}
\input{sections/data}

%%%%%\input{sections/abaltion}

\input{sections/results}



\input{sections/conclusion}

 
\subsubsection*{Acknowledgments}
%This research was supported by Zhipu AI. Thanks to BiliBili for data support. Thanks to all our collaborators and partners from Knowledge Engineering Group (KEG) and Zhipu AI.

% We would like to thank Xiaohan Zhang, Da Yin, Guanyu Feng, Ting Liu, Wei Jia, Jiajun Xu and all the data annotators, infra-operating staff, collaborators, and partners as well as everyone at Zhipu AI and Tsinghua University not explicitly mentioned in the report who have provided support, feedback, and contributed to the \model.
We would like to thank all the data annotators, infrastructure operators, collaborators, and partners. We also extend our gratitude to everyone at Zhipu AI and Tsinghua University who have provided support, feedback, or contributed to the \model, even if not explicitly mentioned in this report.
We would also like to greatly thank BiliBili for technical discussions. 
% We would also like to greatly thank BiliBili for data support. 
% We would also like to thank Yuxuan Zhang and Wei Jia from Zhipu AI as well as the teams at Hugging Face, ModelScope, WiseModel, and others for their help on the open-sourcing efforts of the GLM family of models.


% \section*{References}
\bibliography{reference}
\bibliographystyle{iclr2024_conference}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\appendix
\input{appendix/image2video}
\input{appendix/caption_upsampler}
\input{appendix/video_caption_gen}
\input{appendix/video_caption_example}
\input{appendix/video2video}
\input{appendix/human_evaluation}

\end{document}