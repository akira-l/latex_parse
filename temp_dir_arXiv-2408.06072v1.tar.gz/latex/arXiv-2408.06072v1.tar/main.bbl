\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[pik(2023)]{pika}
Pika beta.
\newblock 2023.
\newblock URL \url{https://pika.art/home}.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{GPT4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[AI@Meta(2024)]{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.
\newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Bai et~al.(2024)Bai, Lv, Zhang, He, Qi, Hou, Tang, Dong, and Li]{bai2024longalign}
Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji~Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.
\newblock Longalign: A recipe for long context alignment of large language models.
\newblock \emph{arXiv preprint arXiv:2401.18058}, 2024.

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and Zisserman]{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  1728--1738, 2021.

\bibitem[Betker et~al.(2023)Betker, Goh, Jing, Brooks, Wang, Li, Ouyang, Zhuang, Lee, Guo, et~al.]{betker2023improving}
James Betker, Gabriel Goh, Li~Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et~al.
\newblock Improving image generation with better captions.
\newblock \emph{Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf}, 2\penalty0 (3):\penalty0 8, 2023.

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, et~al.]{blattmann2023stable}
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et~al.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Zhang, Cun, Xia, Wang, Weng, and Shan]{chen2024videocrafter2}
Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.
\newblock Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Siarohin, Menapace, Deyneka, Chao, Jeon, Fang, Lee, Ren, Yang, et~al.]{chen2024panda}
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung~Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et~al.
\newblock Panda-70m: Captioning 70m videos with multiple cross-modality teachers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  13320--13331, 2024{\natexlab{b}}.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16344--16359, 2022.

\bibitem[Dehghani et~al.(2024)Dehghani, Mustafa, Djolonga, Heek, Minderer, Caron, Steiner, Puigcerver, Geirhos, Alabdulmohsin, et~al.]{dehghani2024patch}
Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim~M Alabdulmohsin, et~al.
\newblock Patch nâ€™pack: Navit, a vision transformer for any aspect ratio and resolution.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{esser2021taming}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  12873--12883, 2021.

\bibitem[Esser et~al.(2024)Esser, Kulal, Blattmann, Entezari, M{\"u}ller, Saini, Levi, Lorenz, Sauer, Boesel, et~al.]{esser2024scaling}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Guo et~al.(2023)Guo, Yang, Rao, Liang, Wang, Qiao, Agrawala, Lin, and Dai]{guo2023animatediff}
Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu~Qiao, Maneesh Agrawala, Dahua Lin, and Bo~Dai.
\newblock Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.
\newblock \emph{arXiv preprint arXiv:2307.04725}, 2023.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Ho et~al.(2022)Ho, Chan, Saharia, Whang, Gao, Gritsenko, Kingma, Poole, Norouzi, Fleet, et~al.]{ho2022imagen}
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik~P Kingma, Ben Poole, Mohammad Norouzi, David~J Fleet, et~al.
\newblock Imagen video: High definition video generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.02303}, 2022.

\bibitem[Hong et~al.(2022)Hong, Ding, Zheng, Liu, and Tang]{hong2022cogvideo}
Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang.
\newblock Cogvideo: Large-scale pretraining for text-to-video generation via transformers.
\newblock \emph{arXiv preprint arXiv:2205.15868}, 2022.

\bibitem[Huang et~al.(2024)Huang, He, Yu, Zhang, Si, Jiang, Zhang, Wu, Jin, Chanpaisit, Wang, Chen, Wang, Lin, Qiao, and Liu]{huang2023vbench}
Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu~Qiao, and Ziwei Liu.
\newblock {VBench}: Comprehensive benchmark suite for video generative models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024.

\bibitem[Li et~al.(2024)Li, Feng, Fu, Wang, Basu, Chen, and Wang]{li2024t2v}
Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William~Yang Wang.
\newblock T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback.
\newblock \emph{arXiv preprint arXiv:2405.18750}, 2024.

\bibitem[Liao et~al.(2024)Liao, Lu, Zhang, Wan, Wang, Zhao, Zuo, Ye, and Wang]{liao2024evaluationtexttovideogenerationmodels}
Mingxiang Liao, Hannan Lu, Xinyu Zhang, Fang Wan, Tianyu Wang, Yuzhong Zhao, Wangmeng Zuo, Qixiang Ye, and Jingdong Wang.
\newblock Evaluation of text-to-video generation models: A dynamics perspective, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.01094}.

\bibitem[Lin et~al.(2024)Lin, Liu, Li, and Yang]{lin2024common}
Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.
\newblock Common diffusion noise schedules and sample steps are flawed.
\newblock In \emph{Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pp.\  5404--5411, 2024.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pp.\  740--755. Springer, 2014.

\bibitem[OpenAI(2024{\natexlab{a}})]{gpt4o}
OpenAI.
\newblock Gpt-4o.
\newblock 2024{\natexlab{a}}.

\bibitem[OpenAI(2024{\natexlab{b}})]{sora}
OpenAI.
\newblock Sora.
\newblock 2024{\natexlab{b}}.
\newblock URL \url{https://openai.com/index/sora/}.

\bibitem[Peebles \& Xie(2023)Peebles and Xie]{peebles2023scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  4195--4205, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[runway(2023)]{gen2}
runway.
\newblock Gen-2.
\newblock 2023.
\newblock URL \url{https://runwayml.com/ai-tools/gen-2-text-to-video}.

\bibitem[Salimans \& Ho(2022)Salimans and Ho]{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock \emph{arXiv preprint arXiv:2202.00512}, 2022.

\bibitem[Singer et~al.(2022)Singer, Polyak, Hayes, Yin, An, Zhang, Hu, Yang, Ashual, Gafni, et~al.]{singer2022make}
Uriel Singer, Adam Polyak, Thomas Hayes, Xi~Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock \emph{arXiv preprint arXiv:2209.14792}, 2022.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Team(2024)]{kling}
Kuaishou~AI Team.
\newblock Kling.
\newblock 2024.
\newblock URL \url{https://kling.kuaishou.com/en}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Villegas et~al.(2022)Villegas, Babaeizadeh, Kindermans, Moraldo, Zhang, Saffar, Castro, Kunze, and Erhan]{villegas2022phenaki}
Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad~Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.
\newblock Phenaki: Variable length video generation from open domain textual descriptions.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Lv, Yu, Hong, Qi, Wang, Ji, Yang, Zhao, Song, et~al.]{wang2023cogvlm}
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et~al.
\newblock Cogvlm: Visual expert for pretrained language models.
\newblock \emph{arXiv preprint arXiv:2311.03079}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Chen, Ma, Zhou, Huang, Wang, Yang, He, Yu, Yang, et~al.]{wang2023lavie}
Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi~Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et~al.
\newblock Lavie: High-quality video generation with cascaded latent diffusion models.
\newblock \emph{arXiv preprint arXiv:2309.15103}, 2023{\natexlab{b}}.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, et~al.]{xiong2023effective}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz, et~al.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{arXiv preprint arXiv:2309.16039}, 2023.

\bibitem[Yu et~al.(2023)Yu, Lezama, Gundavarapu, Versari, Sohn, Minnen, Cheng, Gupta, Gu, Hauptmann, et~al.]{yu2023language}
Lijun Yu, Jos{\'e} Lezama, Nitesh~B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander~G Hauptmann, et~al.
\newblock Language model beats diffusion--tokenizer is key to visual generation.
\newblock \emph{arXiv preprint arXiv:2310.05737}, 2023.

\bibitem[Yuan et~al.(2024)Yuan, Huang, Xu, Liu, Zhang, Shi, Zhu, Cheng, Luo, and Yuan]{yuan2024chronomagic}
Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li~Yuan.
\newblock Chronomagic-bench: A benchmark for metamorphic evaluation of text-to-time-lapse video generation.
\newblock \emph{arXiv preprint arXiv:2406.18522}, 2024.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Wu, Liu, Zhao, Ran, Gu, Gao, and Shou]{zhang2023show}
David~Junhao Zhang, Jay~Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike~Zheng Shou.
\newblock Show-1: Marrying pixel and latent diffusion models for text-to-video generation.
\newblock \emph{arXiv preprint arXiv:2309.15818}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Li, and Bing]{zhang2023video}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for video understanding.
\newblock \emph{arXiv preprint arXiv:2306.02858}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and Wang]{zhang2018unreasonable}
Richard Zhang, Phillip Isola, Alexei~A Efros, Eli Shechtman, and Oliver Wang.
\newblock The unreasonable effectiveness of deep features as a perceptual metric.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  586--595, 2018.

\bibitem[Zheng et~al.(2024{\natexlab{a}})Zheng, Teng, Yang, Wang, Chen, Gu, Dong, Ding, and Tang]{zheng2024cogview3}
Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie Tang.
\newblock Cogview3: Finer and faster text-to-image generation via relay diffusion.
\newblock \emph{arXiv preprint arXiv:2403.05121}, 2024{\natexlab{a}}.

\bibitem[Zheng et~al.(2024{\natexlab{b}})Zheng, Peng, Yang, Shen, Li, Liu, Zhou, Li, and You]{opensora}
Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You.
\newblock Open-sora: Democratizing efficient video production for all, March 2024{\natexlab{b}}.
\newblock URL \url{https://github.com/hpcaitech/Open-Sora}.

\end{thebibliography}
